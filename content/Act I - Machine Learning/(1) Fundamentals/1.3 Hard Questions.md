##### How do you mitigate the problem of class imbalance, and why is it important?

> [!check] Class imbalance
> Occurs when one class has significantly fewer samples compared to others, especially in classification problems. 
> This can cause the model to be biased which favour the majority class, resulting in poor predictive performance for the minority, even if this is disguised in the overall performance metrics.


1. **Collect more data**: While obvious, this should be your first thought!
2. **Resampling**
	1. Oversampling the minority class
	2. Undersampling the majority class
	3. Or both
3. **On the algorithmic level**
	1. Use bagging or boosting methods (Adaboost, Gradient Boosting) - which have better performance on imbalanced datasets
	2. Assign higher misclassification costs to the minority class in order to make the model more 'careful'
4. **Anomaly Detection**: Treat the problem as an anomaly detection task if the minority class can be considered as 'rare events'.
5. **Using Ensemble Methods**: Bagging and boosting can help in reducing the variance caused by imbalances.
6. **Generate Synthetic Data**: Use data augmentation techniques or synthetic data generation methods to create additional samples for the minority class. Note that this only applies to specific types of data. 
7. **Treat it as Anomaly Detection**: For extreme cases where the minority class is so small that it is basically an anomaly. 

The reason class imbalance is a problem is that most machine learning algorithms are designed to optimize overall accuracy, which can be misleading when one class dominates. They tend to predict the majority class for most instances, leading to poor predictive performance, especially for the minority class. 

This is particularly critical in fields like medical diagnosis, fraud detection, or any domain where the minority class is of great interest despite its rarity.


<br>

##### ↳ How can you actually measure a model's performance on imbalanced datasets?

**Evaluation metrics** such as:
- Precision
- Recall 
- F1-Score
- ROC-AUC

<br>


---
##### Could you name some resampling methods? 

- Simply duplicate existing samples
- Bootstrap sampling

> [!check] Bootstrap Sampling
> Read more about the math here 

- SMOTE

> [!check] SMOTE
> Synthetic Minority Oversampling Technique
> 



<br>

---

##### You mentioned bagging and boosting. What is the difference between the two methods?

Bagging is a method of merging the same type of predictions. 
Bagging decreases variance, not bias, and solves over-fitting issues in a model. 

Boosting is a method of merging different types of predictions. 
Boosting decreases bias, not variance.

![[1*zTgGBTQIMlASWm5QuS2UpA-2.jpg]]

Note the **independence** 
Note the **dependence**

![[1*r24_G4jmjpffc8Xqhq2R1g.png]]
<br>

---

##### What is outlier detection? 

>[!check] Anomaly detection
>Also known as outlier detection, is the process of identifying data points, events, or observations that deviate significantly from the dataset's normal behavior.
>**Three types:** point, collective, and contextual anomalies

- Fraud
- Network intrusions
- Structural defects
- Health related issues




<br>

---
##### Question Here


> Data enrichment. Kind of like data linking. Let’s say we have data on a list of company’s. We could enrich this dataset by combining it with a separate dataset with companies and their number of employees. We gain information for free, and thus can produce more powerful models.

---

###### What makes a model actually good? 

What do you mean by ‘powerful model’. 

It actually relates to the statistical definition ‘power’ (link) - the probability that the model is actually correct

Simp. Mew. go

###### How do we measure how good a model is?
#first-princples



<br>

Meaning of semantics. 

<br>

---

##### How do you keep track of a model's performance once it's been deployed? 

- Continuously track **key performance indicators** (KPIs) relevant to the model, such as the usual:: accuracy, precision, recall, F1 score, ROC-AUC... 
- For regression models, metrics like Mean Squared Error (MSE), Root Mean Squared Error (RMSE), or Mean Absolute Error (MAE) are commonly monitored.
- Monitor data drift
- Monitor concept drift
- Routine model retraining 
- Error analysis 

>[!check] Model Drift
> **There are two types:**
> - Concept Drift (the underlying relationship between x and y changes)
> - Data Drift (the underlying distribution of the input changes)

>[!check] Concept Drift (Model Drift)
>Over time, the statistical properties of the target variable, which the model is predicting, can change. This phenomenon, known as model drift or concept drift, can occur due to evolving trends, changing market conditions, or alterations in customer behavior.


<br>


---
##### Why might a model not be performing as well as initially intended? 

An interesting question. This might occur in the following scenarios:
- The model overfit to the training data, and thus performed poorly on the unseen real world data. 
- 



##### The above question but for recommender systems in big apps (netflix/spotify/canva)
##### How do you predict customer revenue? 
##### How can you predict churn? (at different time windows is useful) 
##### We've seen a spike in churn rate, how can you figure out why? (think feature importance) 
##### Deprecated but my Jupyter notebook question was about finding out what characteristics of people are important when determining how good a customer they are (also feature importance) 
##### How can you deal with a categorical FEATURE in a model that only takes numerical inputs? (think one hot encoding, sequential encoding but only if it is appropriate/ordinal variable) 
##### What do you do if you have a LOT of cardinality in the feature? (one hot encoding is a bad idea for something like postcodes, can you group them into states/LGAs/countries etc. instead so they're still useful and not like 10,000 extra binary features)





<br>

---

You're right to question the use of squaring the modulus. In the expression \(\| \epsilon - \hat{\epsilon} \|^2\), the notation \(\|\cdot\|\) usually refers to a norm, often the Euclidean norm in the context of vectors, which itself represents a kind of "distance" or "magnitude." Squaring this norm is a common practice in various fields, including statistics and machine learning, for several reasons:

1. **Emphasizing Larger Errors**: Squaring magnifies larger differences more than smaller ones. This can be useful when the aim is to give more weight to larger errors.

2. **Analytical Convenience**: Squaring the norm often leads to mathematical expressions that are easier to differentiate and work with, especially in optimization problems.

3. **Removing Sign**: The square ensures that the result is always non-negative, regardless of the sign of \(\epsilon - \hat{\epsilon}\). This is useful when only the magnitude of the error matters, not its direction.

4. **Euclidean Distance**: When \(\epsilon\) and \(\hat{\epsilon}\) are vectors, \(\| \epsilon - \hat{\epsilon} \|\) represents the Euclidean distance between them, and squaring this norm gives the square of the Euclidean distance, which is a common measure in various algorithms.

So, the expression \(\| \epsilon - \hat{\epsilon} \|^2\) is indeed meaningful and correct depending on the context in which it is used. It represents the square of the norm (or distance) between \(\epsilon\) and \(\hat{\epsilon}\).

<br>

---
##### What is the "quality versus inference budget" tradeoff in ML? 

> [!check] Quality vs. Budget Tradeoff
> The "quality versus inference budget tradeoff" is a concept often encountered in the field of machine learning, particularly in the context of deploying models in real-world applications. This tradeoff revolves around balancing the quality of the model's predictions (accuracy, precision, reliability, etc.) with the resources required to generate these predictions (time, computational power, cost, etc.). Here's a more detailed explanation:

<br>

---

##### What is the "bias versus variance" tradeoff in ML? 

***Here's a few definitions to chew on.***

> [!check] Bias vs. Variance Tradeoff 
> The tradeoff between fitting the **training data** well and performing well on **real/testing** data. 

> [!check] Bias vs. Variance Tradeoff
> In statistics and machine learning, the bias–variance tradeoff describes the relationship between a model's complexity, the accuracy of its predictions, and how well it can make predictions on previously unseen data that were not used to train the model.

It's essentially saying; you can learn all the patterns in the data, but at some point you'll be learning random noise (unhelpful) specific to this particular training data, instead of useful signal that will be present in any real data. 

**The job of every single machine learning model is to learn patterns in the data.**
You want a sufficiently sophisticated model that it can learn these tiny clever little patterns in the data but eventually you can go too far and into learning just pure noise, the model thinks its being all cleer and whatnot but


An optimized model will be sensitive to the patterns in our data, but at the same time will be able to generalize to new data.

