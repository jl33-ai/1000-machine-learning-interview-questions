*These questions were curated from the application process of large software development companies such as **Google, Atlassian, Canva, and Amazon.***

- How to maintain in deploymnet
- Cross validation 
Models (from the 42 + stat question) 
- Logistic
- SVM
- Linear Regression + 
- Grid Search 
- Regularisation 
	- Lasso
	- Ridge 

- what is machine learning? 
- Machine learning is autocorrect - think, anything that would be hard to explicitly program... sometimes you don't realise until you try. 


These sections will take you through the terminology and fundamental building blocks
Things which are the building blocks for the following models. Which you must understand before you explore all of them. 
This is the language which will allow you to become fluent in machine learning, no matter which of the following X model's we're discussing. 
That said, they're sometimes hard to understadn without reference to a specific example model. 

- Training/Testing 
	- the real meaning of 'training'
- 'train-test split'
	- More precisely, training = estimating parameters
	- More precisely, testing = 
- Cross validation
- Confusion matrix
- The data (just covered)
- Bias Variance tradeoff 


- Loss function
- Inference - a general concept used. You may hear the term 'running inference on a model' that has just been trained. It essentially just means, having trained the model, to **use it** in order to generate classifications or predictions. 
- Explanaibility
- Corpus

ChatGPT 'predicts' the next best word. 

- ROC curve 
- AOC curve


- Functions
- Log function
- Prob and Stats basics (normal distribution) and (basic stats)


A/B testing

Accuracy
Binary Classification

Data augmentation
Implicit Bias
Multi class classification
TN, TP, TPR, Validation Set, 
Variable imporance

Few shot learning
Hyperparameter




[[(1.1) Easy Questions]]
[[(1.3) Normal Distribution]]



---

###### What is Machine Learning?

> â˜ž **Machine Learning**: How to teach a computer a task, without explicitly programming it to perform said task. Instead, feed data into an algorithm to gradually improve outcomes through experience. 

<br>

---

###### What two different types of tasks do all machine learning models do?

> â˜ž **Classification** of present data into categories
$f(\text{data})â†’\text{classification}$

> â˜ž **Prediction** about the future
$f(\text{data})â†’\text{prediction}$

regression? 

<br>

---



---

###### (A) Learning paradigms
- Supervised Learning
	- Classification
	- Prediction/Regression
- Unsupervised Learning
	- Clustering
- Reinforcement Learning 
	- Rewarding



	- Dimensionality Reduction
	- Generative AI 
	- Active Learning
	- Ensemble Methods 
<br>

---

###### The Current ML Taxonomy
- Computer Vision 
- NLP 

<br>

---

###### What is active learning? 

<br>

---





<br>


---

###### Q1) Let's say you want to make a model. What are the general steps you take? 
**First principles**

The holistic machine learning recipe (chant this every night before you go to sleep): 
1. **Acquire data**
2. **Clean and prepare data** into good features
3. **Split the data** into **training** and **testing** set
4. **Pick a model** (the fun part)
	1. **Traditional Statistical models:** linear regression, logistic regression
	2. **ML Model:** decision tree, kNN
	3. **Deep Learning:** cNN{algorithm which assigns weight to features, takes input, automatically takes additional features - helpful for images or Natural language where manual feature  engineering}
5. **Train:** train data is fed into algorithm to build model

> â˜ž In machine learning, ***training the algorithm*** can be thought of as ***learning*** which is really just ***estimating the parameters of the model***

6. **Test:** test data used to evaluate the accurate or error of the model

<br>

---

###### Q2) What is a function?

> â˜ž **Function**: A function is a machine that takes in an input and maps it to an output.

If you have a reasonable understanding of machine learning, you can appreciate that all 'machine learning models' are really just super complicated 'functions' which take in data and spit out a result
- The same way the function $f(x) = x^2$ takes in the **number** $x$, and returns the **number squared**
- Or the function below takes in a **list**, and returns the **list sorted**

```python
def sort_list(my_list):
	return sorted(my_list)
```

 - A machine learning model takes in **data** and returns an **answer**, but often both the process of going from input to output is much more complicated than a simple $y=x^2$
 - You should really think about most machine learning models as plain old **functions**

| | $x$ | â†’ | f(x) | 
|---|--|---|---|
|ChatGPT|Prompt | |Response |



- plain and simple. 
- It is quite 
- The better the data represents the problem, the better the results. 
- 'You are what you eat'
- The model is trained on the data. If the data is fundamentally wrong, then the model will still do the right thing - it may learn the data **perfectly** - but as a result, the model's output is also wrong due to the data being wrong. 
- Remember how the model uses data to estimate its parameters so that it can adjust itself to perform well on unseen data? Clearly, bad data will lead to poor estimates of these parameters. I should be clean, and representative of the unseen data that will be fed into it; if you're trying to train a model to recognise dog breeds, you wouldn't feed it pictrues of trains would you. 



<br>


---

<br> 

---

###### QX) What are the two types of data? 



<br>

---

###### QX) What is metadata with an example? 

> â˜ž Metadata is data that describes other data.

**Example** 
When you take a photo on your phone, the details that come up when you press (â„¹ï¸Ž) are metadata describing the main data - the image.  
`Time_taken: 2:03:29am`
`Location: The White House`
`Size: 16MP`
`Dimensions: 1440x1440`

<br>

*Metadata is relevant to machine learning because it provides context, aids in data preprocessing, enhances data quality, and can improve model interpretability and performance.*

<br>

---

###### Q1) **Explain the concept of 'garbage in garbage Out' and motivate with an example**


<br> 

---

###### Q1) What is a feature? 



###### Featuring engineering is one of a Data Scientist's jobs. What is it? 

The features must contain some kind of **signal** that is valuable to the algorithm for its classification/prediction

So the raw data must be transformed into new features that better represent the underlying problem. 

<br> 

---


---

###### Q7: **What is supervised ML?**
- New book is good 

**Supervised learning**: Supervised Machine Learning refers to a type of machine learning where the algorithm is trained on a labeled dataset, which means the algorithm is provided with input-output pairs. The goal is to learn a mapping from inputs to outputs and make predictions on new, unseen data based on this learned mapping.

**Unsupervised learning**Â does not have (or need) any labeled outputs, so its goal is to infer the natural structure present within a set of data points.

**Semi-supervised learning**Â aims to label unlabeled data points using knowledge learned from a small number of labeled data points.

**Reinforcement learning** 

<br>


---

###### What is overfitting? 

---

###### What is underfitting?

---


---

###### What metrics do we use to evaluate the effectiveness of a machine learning model?

> 	â˜ž In statistics we are usually evaluating 'estimators' of population parameters; e.g the population mean, the true regression line. 
> â˜ž In machine learning we evaluate entire models, which estimate the entire model 

**Bias**

**Variance**

**Precision**
1. \% of model's positive classifications that were correct (ML)
2. Reciprocal of the variance (Stats)

**Accuracy**

**Recall**



<br>

---

###### What is a loss function, heuristically, holistically, in general, what is the ideology of a loss function?

###### What metrics do we use to evaluate the effectiveness of a statistical estimator?

- Bias
- Variance
- Efficiency
- Consistency
- Precision

> Most statistical models are also classified as machine learning models (e.g Linear Regression) so you can see how quickly things get confusing! 

Every machine learning model learns to get better by comparing its output to an **error/loss function**
- For classification problems (categorical)
	- Accuracy (how many did you get wrong)
- For regression problems (numeric)
	- Mean squared error 



---

**The Confusion Matrix**

---

###### Cross Validation
#cross_validation




---


###### QX) What is overfitting?

In the context of machine learning models, overfitting is when the model 'learns' the **training data** too well, resulting in almost no bias, but high variance. 

**Let's say** *Tao* is studying (learning) for an upcoming exam. 

- **Memorizing past exam papers** is akin to a model that overfits to its training data.
- **New questions on the exam** represent new, unseen data for a predictive model.
- **Genuine understanding of the material** is similar to a well-generalized model that can perform well on both training and unseen data.

###### Follow up: what would an overfit ? 

<br>
<br>

---

###### What is cross-validation? 

<br>

---

###### Bias/Variance tradeoff 

<br> 

---

###### What is inference?

> â˜ž **ðŒð‹ inference** is basically just the act of actually **using the trained model** on new, unseen data. It is the post-training step where the model applies its learned knowledge to provide outputs for inputs it **hasn't encountered during training**. Essentially, it's where the model is put to work in practical applications, such as predicting outcomes, classifying data, or generating content.

> **Statistical inference** is the process of drawing conclusions about a **larger population** or **underlying data process** based on a sample from that population. E.g what is the mean weight of boys at your school, based on the mean weight of boys in your class âœ».

<div class="asterix">âœ» Please never do this</div>

<br>

---




###### QX) **Explain a machine learning project you have worked on in the past. Why did you choose that specific model and how did you prepare your data?**
   - **A**: I worked on a sentiment analysis project to determine customer feedback polarity. I chose the Naive Bayes model due to its efficiency with textual data. For data preparation, I tokenized the reviews, removed stopwords, and used TF-IDF for feature extraction.

<br>

---

---

<br>

---

