*These questions were curated from the application process of large software development companies such as **Google, Atlassian, Canva, and Amazon.***


For Data Science refer to [[2.4 pandas]]

- How to maintain in deploymnet
- Cross validation 
Models (from the 42 + stat question) 
- Logistic
- SVM
- Linear Regression + 
- Grid Search 
- Regularisation 
	- Lasso
	- Ridge 

- what is machine learning? 
- Machine learning is autocorrect - think, anything that would be hard to explicitly program... sometimes you don't realise until you try. 


These sections will take you through the terminology and fundamental building blocks
Things which are the building blocks for the following models. Which you must understand before you explore all of them. 
This is the language which will allow you to become fluent in machine learning, no matter which of the following X model's we're discussing. 
That said, they're sometimes hard to understadn without reference to a specific example model. 

- Training/Testing 
	- the real meaning of 'training'
- 'train-test split'
	- More precisely, training = estimating parameters
	- More precisely, testing = 
- Cross validation
- Confusion matrix
- The data (just covered)
- Bias Variance tradeoff 


- Loss function
- Inference - a general concept used. You may hear the term 'running inference on a model' that has just been trained. It essentially just means, having trained the model, to **use it** in order to generate classifications or predictions. 
- Explanaibility
- Corpus

ChatGPT 'predicts' the next best word. 

- ROC curve 
- AOC curve


- Functions
- Log function
- Prob and Stats basics (normal distribution) and (basic stats)


A/B testing

Accuracy
Binary Classification

Data augmentation
Implicit Bias
Multi class classification
TN, TP, TPR, Validation Set, 
Variable imporance

Few shot learning
Hyperparameter




[[(1.1) Easy Questions]]
[[(1.3) Normal Distribution]]



<br>

---
##### What is a parameter

> [!check] A parameter
> Is a weight learnt by the model during training




<br>


---

##### What is data pre-processing

> [!check] Preprocessing
> All the steps that need to happen before the data can be 'fed' to the model. 

##### What is data imputation? 

##### Standardisation vs. Normalisation

###### ↳ What are some different ways to impute missing data? 


---

##### Q2) What is a function?

> ☞ **Function**: A function is a machine that takes in an input and maps it to an output.

If you have a reasonable understanding of machine learning, you can appreciate that all 'machine learning models' are really just abstracted 'functions' which take in data and spit out a result
- The same way the function $f(x) = x^2$ takes in the **number** $x$, and returns the **number squared**
- Or the function below takes in a **list**, and returns the **list sorted**

```python
def sort_list(my_list):
	return sorted(my_list)
```

 - A machine learning model takes in **data** and returns an **answer**, but often both the process of going from input to output is much more complicated than a simple $y=x^2$
 - You should really think about most machine learning models as plain old **functions**

| | $x$ | → | f(x) | 
|---|--|---|---|
|ChatGPT|Prompt | |Response |



- plain and simple. 
- It is quite 
- The better the data represents the problem, the better the results. 
- 'You are what you eat'
- The model is trained on the data. If the data is fundamentally wrong, then the model will still do the right thing - it may learn the data **perfectly** - but as a result, the model's output is also wrong due to the data being wrong. 
- Remember how the model uses data to estimate its parameters so that it can adjust itself to perform well on unseen data? Clearly, bad data will lead to poor estimates of these parameters. I should be clean, and representative of the unseen data that will be fed into it; if you're trying to train a model to recognise dog breeds, you wouldn't feed it pictrues of trains would you. 



<br>


---

<br> 

---

###### QX) What are the two types of data? 

Structured

Unstructured

A good proxy for how 'structured' something is, is how easy it would be to represent the data in a table. I.e: very hard for images/videos, very easy for a company's expenses


<br>

---

###### QX) What is metadata with an example? 

> ☞ Metadata is data that describes other data.

**Example** 
When you take a photo on your phone, the details that come up when you press (ℹ︎) are metadata describing the main data - the image.  
`Time_taken: 2:03:29am`
`Location: The White House`
`Size: 16MP`
`Dimensions: 1440x1440`

<br>

*Metadata is relevant to machine learning because it provides context, aids in data preprocessing, enhances data quality, and can improve model interpretability and performance.*

<br>

---

###### Q1) **Explain the concept of 'garbage in garbage Out' and motivate with an example**


<br> 

---

###### Q1) What is a feature? 



###### Featuring engineering is one of a Data Scientist's jobs. What is it? 

The features must contain some kind of **signal** that is valuable to the algorithm for its classification/prediction

So the raw data must be transformed into new features that better represent the underlying problem. 

<br> 

---


---
<br>


---

###### What is overfitting? 

A model that is overfit has low bias (good) and high variance (bad). 
This happens when the model learns noise and fluctuations in the training data as if they were significant features.
It thus performs poorly on real, unseen data. 

<br>

---

###### What is underfitting?

---


---

###### What evaluation metrics do we use to evaluate the effectiveness of a machine learning model?

> 	☞ In statistics we are usually evaluating 'estimators' of population parameters; e.g the population mean, the true regression line. 
> ☞ In machine learning we evaluate entire models, which estimate the entire model 


---

> [!check] Precision
> The % of positive classifications that were correct


**Accuracy**

**Recall**



<br>

---

###### What is a loss function, heuristically, holistically, in general, what is the ideology of a loss function?

###### What metrics do we use to evaluate the effectiveness of a statistical estimator?

- Bias
- Variance
- Efficiency
- Consistency
- Precision

> Most statistical models are also classified as machine learning models (e.g Linear Regression) so you can see how quickly things get confusing! 

Every machine learning model learns to get better by comparing its output to an **error/loss function**
- For classification problems (categorical)
	- Accuracy (how many did you get wrong)
- For regression problems (numeric)
	- Mean squared error 



---

**The Confusion Matrix**

---

###### Cross Validation
#cross_validation




---


###### QX) What is overfitting?

In the context of machine learning models, overfitting is when the model 'learns' the **training data** too well, resulting in almost no bias, but high variance. 

**Let's say** *Tao* is studying (learning) for an upcoming exam. 

- **Memorizing past exam papers** is akin to a model that overfits to its training data.
- **New questions on the exam** represent new, unseen data for a predictive model.
- **Genuine understanding of the material** is similar to a well-generalized model that can perform well on both training and unseen data.

###### Follow up: what would an overfit ? 

<br>
<br>

---

###### What is cross-validation? 

<br>

---

###### Bias/Variance tradeoff 

- Bias: measures the tendency of the model to fit the **training data**
- Variance: this type of model tends (graph)




<br> 

---

##### How is this different to statistical bias/variance

In statistics, an estimator (model) is unbiased if, in the long run, its average value is equal to the **true underlying value**.

A model's variance measures how varied its results are 

<br>

---
###### What is inference?

> ☞ **𝐌𝐋 inference** is basically just the act of actually **using the trained model** on new, unseen data. It is the post-training step where the model applies its learned knowledge to provide outputs for inputs it **hasn't encountered during training**. Essentially, it's where the model is put to work in practical applications, such as predicting outcomes, classifying data, or generating content.

> **Statistical inference** is the process of drawing conclusions about a **larger population** or **underlying data process** based on a sample from that population. E.g what is the mean weight of boys at your school, based on the mean weight of boys in your class ✻.

<div class="asterix">✻ Please never do this</div>

<br>

---




###### QX) **Explain a machine learning project you have worked on in the past. Why did you choose that specific model and how did you prepare your data?**
   - **A**: I worked on a sentiment analysis project to determine customer feedback polarity. I chose the Naive Bayes model due to its efficiency with textual data. For data preparation, I tokenized the reviews, removed stopwords, and used TF-IDF for feature extraction.

<br>

---




<br> 



---

<br>

---

##### What is data leakage in cross validation? 





##### Data; dirty data, high variance data, garbage in garbage out, signal

<br>

---


##### Train vs. Validate vs. Test

<br>

---

##### What is a class in ML? 

> [!check] Class
> A class is another name for a category. 

> [!check] Classification
> A datapoints's classification is the category that it belongs to. 

<br>

---

##### What is a 'dimension' of data in ML

A degree of freedom which specifies the value of the datapoint (think: how many different would you need to fully specify the data) - you couldn't specify the shape of a phone with just one number, you would need height and width. 

A loose proxy; just think number of columns in the dataset

###### ↳ Can you give an example of N-dimensional data? 

