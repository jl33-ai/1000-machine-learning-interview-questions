##### How do you mitigate the problem of class imbalance, and why is it important?

> [!check] Class imbalance
> Occurs when one class has significantly fewer samples compared to others, especially in classification problems. 
> This can cause the model to be biased which favour the majority class, resulting in poor predictive performance for the minority, even if this is disguised in the overall performance metrics.


1. **Collect more data**: While obvious, this should be your first thought!
2. **Resampling**
	1. Oversampling the minority class
	2. Undersampling the majority class
	3. Or both
3. **On the algorithmic level**
	1. Use bagging or boosting methods (Adaboost, Gradient Boosting) - which have better performance on imbalanced datasets
	2. Assign higher misclassification costs to the minority class in order to make the model more 'careful'
4. **Anomaly Detection**: Treat the problem as an anomaly detection task if the minority class can be considered as 'rare events'.
5. **Using Ensemble Methods**: Bagging and boosting can help in reducing the variance caused by imbalances.
6. **Generate Synthetic Data**: Use data augmentation techniques or synthetic data generation methods to create additional samples for the minority class. Note that this only applies to specific types of data. 
7. **Treat it as Anomaly Detection**: For extreme cases where the minority class is so small that it is basically an anomaly. 

The reason class imbalance is a problem is that most machine learning algorithms are designed to optimize overall accuracy, which can be misleading when one class dominates. They tend to predict the majority class for most instances, leading to poor predictive performance, especially for the minority class. 

This is particularly critical in fields like medical diagnosis, fraud detection, or any domain where the minority class is of great interest despite its rarity.


<br>

##### ↳ How can you actually measure a model's performance on imbalanced datasets?

**Evaluation metrics** such as:
- Precision
- Recall 
- F1-Score
- ROC-AUC

<br>

---

##### harder ones

1. Precisiona t. 
1. **Mean Average Precision (MAP)**: This is used to compute the average precision value for recall value over a certain threshold. It's an extension of precision at k and is particularly useful in information retrieval.
    
2. **Confusion Matrix**: A confusion matrix is a table used to describe the performance of a classification model on a set of test data for which the true values are known. It includes true positives, false positives, true negatives, and false negatives, which are essential for understanding the model's performance.
    
3. **Mean Reciprocal Rank (MRR)**: MRR is a statistic measure for evaluating any process that produces a list of possible responses to a sample of queries, ordered by probability of correctness.
    
4. **Normalized Discounted Cumulative Gain (NDCG)**: Used in information retrieval, NDCG measures the effectiveness of a model by looking at the ranking quality, taking into account the position of the correct labels.
    
5. **Precision-Recall Curve**: This is a plot that shows the trade-off between precision and recall for different thresholds. A high area under the curve represents both high recall and high precision.



<br>

---
##### Could you name some resampling methods? 

- Simply duplicate existing samples
- Bootstrap sampling

> [!check] Bootstrap Sampling
> Read more about the math here 

- SMOTE

> [!check] SMOTE
> Synthetic Minority Oversampling Technique
> 



<br>

---

##### You mentioned bagging and boosting. What is the difference between the two methods?

Bagging is a method of merging the same type of predictions. 
Bagging decreases variance, not bias, and solves over-fitting issues in a model. 

Boosting is a method of merging different types of predictions. 
Boosting decreases bias, not variance.

![[1*zTgGBTQIMlASWm5QuS2UpA-2.jpg]]

Note the **independence** 
Note the **dependence**

![[1*r24_G4jmjpffc8Xqhq2R1g.png]]
<br>

---

##### What is outlier detection? 

>[!check] Anomaly detection
>Also known as outlier detection, is the process of identifying data points, events, or observations that deviate significantly from the dataset's normal behavior.
>**Three types:** point, collective, and contextual anomalies

- Fraud
- Network intrusions
- Structural defects
- Health related issues




<br>

---
##### Question Here


> Data enrichment. Kind of like data linking. Let’s say we have data on a list of company’s. We could enrich this dataset by combining it with a separate dataset with companies and their number of employees. We gain information for free, and thus can produce more powerful models.

---

###### What makes a model actually good? 

What do you mean by ‘powerful model’. 

It actually relates to the statistical definition ‘power’ (link) - the probability that the model is actually correct

Simp. Mew. go

###### How do we measure how good a model is?
#first-princples



<br>

Meaning of semantics. 

<br>

---

##### How do you keep track of a model's performance once it's been deployed? 

- Continuously track **key performance indicators** (KPIs) relevant to the model, such as the usual:: accuracy, precision, recall, F1 score, ROC-AUC... 
- For regression models, metrics like Mean Squared Error (MSE), Root Mean Squared Error (RMSE), or Mean Absolute Error (MAE) are commonly monitored.
- Monitor data drift
- Monitor concept drift
- Routine model retraining 
- Error analysis 

>[!check] Model Drift
> **There are two types:**
> - Concept Drift (the underlying relationship between x and y changes)
> - Data Drift (the underlying distribution of the input changes)

>[!check] Concept Drift (Model Drift)
>Over time, the statistical properties of the target variable, which the model is predicting, can change. This phenomenon, known as model drift or concept drift, can occur due to evolving trends, changing market conditions, or alterations in customer behavior.


<br>


---
##### Why might a model not be performing as well as initially intended? 

An interesting question. This might occur in the following scenarios:
- The model overfit to the training data, and thus performed poorly on the unseen real world data. 
- 



##### The above question but for recommender systems in big apps (netflix/spotify/canva)
##### How do you predict customer revenue? 
##### How can you predict churn? (at different time windows is useful) 
##### We've seen a spike in churn rate, how can you figure out why? (think feature importance) 
##### Deprecated but my Jupyter notebook question was about finding out what characteristics of people are important when determining how good a customer they are (also feature importance) 
##### How can you deal with a categorical FEATURE in a model that only takes numerical inputs? (think one hot encoding, sequential encoding but only if it is appropriate/ordinal variable) 
##### What do you do if you have a LOT of cardinality in the feature? (one hot encoding is a bad idea for something like postcodes, can you group them into states/LGAs/countries etc. instead so they're still useful and not like 10,000 extra binary features)





<br>

---

You're right to question the use of squaring the modulus. In the expression \(\| \epsilon - \hat{\epsilon} \|^2\), the notation \(\|\cdot\|\) usually refers to a norm, often the Euclidean norm in the context of vectors, which itself represents a kind of "distance" or "magnitude." Squaring this norm is a common practice in various fields, including statistics and machine learning, for several reasons:

1. **Emphasizing Larger Errors**: Squaring magnifies larger differences more than smaller ones. This can be useful when the aim is to give more weight to larger errors.

2. **Analytical Convenience**: Squaring the norm often leads to mathematical expressions that are easier to differentiate and work with, especially in optimization problems.

3. **Removing Sign**: The square ensures that the result is always non-negative, regardless of the sign of \(\epsilon - \hat{\epsilon}\). This is useful when only the magnitude of the error matters, not its direction.

4. **Euclidean Distance**: When \(\epsilon\) and \(\hat{\epsilon}\) are vectors, \(\| \epsilon - \hat{\epsilon} \|\) represents the Euclidean distance between them, and squaring this norm gives the square of the Euclidean distance, which is a common measure in various algorithms.

So, the expression \(\| \epsilon - \hat{\epsilon} \|^2\) is indeed meaningful and correct depending on the context in which it is used. It represents the square of the norm (or distance) between \(\epsilon\) and \(\hat{\epsilon}\).

<br>

---
##### What is the "quality versus inference budget" tradeoff in ML? 

> [!check] Quality vs. Budget Tradeoff
> The "quality versus inference budget tradeoff" is a concept often encountered in the field of machine learning, particularly in the context of deploying models in real-world applications. This tradeoff revolves around balancing the quality of the model's predictions (accuracy, precision, reliability, etc.) with the resources required to generate these predictions (time, computational power, cost, etc.). Here's a more detailed explanation:

<br>

---

##### What is the "bias versus variance" tradeoff in ML? 

***Here's a few definitions to chew on.***

> [!check] Bias vs. Variance Tradeoff 
> The tradeoff between fitting the **training data** well and performing well on **real/testing** data. 

> [!check] Bias vs. Variance Tradeoff
> In statistics and machine learning, the bias–variance tradeoff describes the relationship between a model's complexity, the accuracy of its predictions, and how well it can make predictions on previously unseen data that were not used to train the model.

It's essentially saying; you can learn all the patterns in the data, but at some point you'll be learning random noise (unhelpful) specific to this particular training data, instead of useful signal that will be present in any real data. 

**The job of every single machine learning model is to learn patterns in the data.**
You want a sufficiently sophisticated model that it can learn these tiny clever little patterns in the data but eventually you can go too far and into learning just pure noise, the model thinks its being all cleer and whatnot but


An optimized model will be sensitive to the patterns in our data, but at the same time will be able to generalize to new data.

<br>

---
##### False/True Positive/Negative 

<br>

---
##### The concept of signal will change the way you see the world

<br>

---
##### What things do we need to measure the 'distance' between in Machine Learning

In machine learning, measuring distances between different types of data is crucial for various algorithms and models. Here are some conceptual things where distance measurement is important, along with examples of machine learning models where these measures play a significant role:

1. **Text Data:**
   - **Models:**
     - **Natural Language Processing (NLP) Models:** These include models like BERT, GPT, and Transformer-based architectures where text embeddings are compared using distance metrics to understand semantic similarity.
     - **Text Classification Models:** Such as Naive Bayes or Support Vector Machines (SVMs) that may use distance measures in feature space for classification tasks.

2. **Feature Vectors in General:**
   - **Models:**
     - **K-Nearest Neighbors (KNN):** A distance-based classifier where the class of a sample is determined by the classes of its nearest neighbors in the feature space.
     - **Clustering Algorithms:** Like K-means or Hierarchical clustering, where distance measures are used to group similar data points.

3. **Images:**
   - **Models:**
     - **Convolutional Neural Networks (CNNs):** Used in image recognition and classification tasks. Distance measures can be used in the feature space after convolutional layers.
     - **Image Retrieval Systems:** Where distances between image features are calculated for finding similar images.

4. **Time Series Data:**
   - **Models:**
     - **Dynamic Time Warping (DTW):** Used in time series analysis, especially for speech recognition, where it measures the distance between two temporal sequences which may vary in speed.
     - **Recurrent Neural Networks (RNNs) and Long Short-Term Memory Networks (LSTMs):** Used for analyzing time series data where distance metrics can be applied to the feature vectors in the hidden layers.

5. **Graph Data:**
   - **Models:**
     - **Graph Neural Networks (GNNs):** Used in tasks like social network analysis, where distance measures can help in understanding the relationships and influence between nodes.
     - **Network Analysis Tools:** Employed in analyzing and visualizing graph data, where distance metrics can be crucial for clustering and community detection.

6. **Multidimensional Data:**
   - **Models:**
     - **Principal Component Analysis (PCA):** Used for dimensionality reduction, where distances in the high-dimensional space are preserved in the lower-dimensional representation.
     - **Manifold Learning Techniques:** Like t-SNE or UMAP, which rely on distance measures to project high-dimensional data onto lower dimensions while preserving the structure of the data.


##### ↳ Distance measures

In machine learning and data analysis, a variety of distance measures are used to quantify the similarity or dissimilarity between data points. Here are some commonly used distance measures:

1. **Euclidean Distance:**
   - The most common distance metric, it represents the straight line distance between two points in Euclidean space. It's used extensively in clustering, classification, and regression problems.

2. **Manhattan Distance (Taxicab or City Block Distance):**
   - Measures the distance between two points in a grid-based path. It's the sum of the absolute differences of their Cartesian coordinates, and is often used in urban settings where paths are grid-like.

3. **Cosine Similarity:**
   - Measures the cosine of the angle between two non-zero vectors. It's particularly useful in high-dimensional positive spaces, like in text analysis and natural language processing, where it measures the orientation, not magnitude, of vectors.

4. **Hamming Distance:**
   - Used for categorical data, it's a measure of the minimum number of substitutions required to change one string into the other, or the number of positions at which the corresponding symbols are different.

5. **Jaccard Similarity (Jaccard Index):**
   - Used for comparing the similarity and diversity of sample sets. It measures the similarity between finite sets and is defined as the size of the intersection divided by the size of the union of the sample sets.

6. **Mahalanobis Distance:**
   - A measure of the distance between a point and a distribution. Unlike Euclidean distance, it takes into account the correlations of the data set and is scale-invariant. It's often used in multivariate anomaly detection.

7. **Minkowski Distance:**
   - A generalization of the Euclidean and Manhattan distances. It's used in various machine learning algorithms, especially in normed vector spaces.

8. **Levenshtein Distance (Edit Distance):**
   - Measures the number of single-character edits (insertions, deletions, or substitutions) required to change one word into the other. It's widely used in applications like spell checking, DNA sequencing, and natural language processing.

9. **Pearson Correlation Coefficient:**
   - Measures the linear correlation between two variables. It's a value between +1 and −1, where 1 is total positive linear correlation, 0 is no linear correlation, and −1 is total negative linear correlation.

10. **Dice Coefficient:**
    - Similar to the Jaccard Index but emphasizes the intersection over the union. It's used for comparing the similarity of two samples.

##### When would Manhattan similarity be useful? 

<br>

---

##### Name some more evaluation metrics

**Log loss:** Negative log likelihood. 

**Mean squared error:** For each observation, take the difference from the true value, square it, and then take the average across all these squared differences. 

**$R^2$:** How far away is each point from the line of best fit, normalised against

<br>

---

##### Every machine learning model has two things: 1. evaluation metrics and 2. 



