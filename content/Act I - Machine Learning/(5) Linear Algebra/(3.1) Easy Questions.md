


 

###### **What is a tensor?** 

###### **What does it mean for something to be a 'linear combination' of something else?**

###### What makes 2 or more vectors 'linearly independent'

###### What is a 'basis'

###### Do the percentage flip thing and ask to prove how

###### What is the form of a weighted average? 

###### Normalised weight (0-1) and 




###### State the eigenvector/eigenvalue equation

$$A \mathbf{v} = \lambda \mathbf{v}$$

Where:
- $A$ is a square matrix.
- $\mathbf{v}$ is an eigenvector of $A$
- $\lambda$ is the eigenvalue corresponding to the eigenvector $\mathbf{v}$

###### What is it actually saying? 

This equation says that you can post-multiply matrix $A$ by the vector v, and the result is a scalar multiple of vector v again. 
###### Explain why its so important for Machine Learning and Deep Learning

- General optimisation; 
- Regularisation
- This equation is used in PCA in order to determine the principal components that capture the most variance in the data. 
- Finding the steady state distribution of a Markov Chain

<br> 
###### **What is a vector?**



###### QX) What is invariance?

Certainly! Here's a concise explanation of invariance in statistics, computer science, and machine learning:

1. **Statistics**:
   - **Invariance Principle**: It relates to estimators that retain their properties even when subjected to certain transformations.
   - **Example**: Consider the median as an estimator of central location. If you add a constant to each data point in a dataset, the median of the transformed data will be the median of the original data plus the constant. This makes the median invariant to shifts.
   - The variance is **LOCATION INVARIANT** (sketch)
   - The MLE is invariant: 
   - A function of a sufficient statistic is also sufficient 
   - Variance in invariant 
   - **Formula**: If \(M\) is the median of dataset \(X\), then the median of \(X + c\) (where \(c\) is a constant) is $M + c$.


2. **Machine Learning**:
   - **Invariance**: A model's ability to provide consistent outputs despite certain transformations to the input.
   - **Example**: A convolutional neural network (CNN) used for image recognition might be designed to recognize a cat whether it's zoomed-in, rotated, or shifted, making the CNN invariant to these transformations.
   - **Formula**: Let \(f\) be a model. If $f(x) = f(T(x))$ for a transformation \(T\) (like rotation or scaling), then \(f\) is invariant to \(T\).

I hope this provides a clear understanding of invariance in these domains!


<br>

---



###### **What does it mean for something to be 'proportional to' something else?** 

- covariance matrices 

---

- eigenvalues/vectors - what is the logic behind 

$$Av = \lambda v$$
$v$: The **eigenvector** (a vector)
$A$: The **matrix** which acts on $v$ to 'transform it'
$\lambda$: The **eigenvalue** (a scalar number)

So you find $v$ and $\lambda$ **for** a given matrix $A$, such that transforming (multiplying) the vector by $A$ does not change its direction, only its magnitude (by an amount $\lambda$. 



<br>

---

- PCA
- Vector space 


- What Python library would you use for matrix manipulation
- How are matrices used in page rankÂ 
- How can matrices be Used to measure the similarity of two images
- How are matrices used in advanced recommender systems
- How is linear algebra used in a CNN? 
	- The kernel slide thing (do a dot product practice)

