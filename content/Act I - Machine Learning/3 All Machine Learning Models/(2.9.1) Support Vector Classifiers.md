**Helpful to have already answered:**
☞ [[1.3 Hard Questions#What is the "bias versus variance" tradeoff in ML?|The bias/variance tradeoff]]
☞ [[1.3 Hard Questions#cross valida]]|Cross validation]

---
##### What is a Maximal Margin Classifier?  

> [!check] Maximal Margin Classifier
> **The idea** is to simply draw a line through the data which separates it into two classes. A maximal margin classifier simply takes the halfway point between `class_a` and `class_b`, whereas a Support Vector Classifier uses a more robust approach, by allowing for some misclassifications. 
> 
> **A few more definitions:**
☞ The maximal margin classifier is a **crude, hypothetical** model which motivates the need for the Support Vector Classifier. 
☞ The result of using a threshold which maximises the margin.
☞ The maximal margin classifier is the hyperplane with the maximum margin, $max(M)$ subject to $||β||=1$

<br>

---
##### What is a Margin?

>[!check] Margin
> The margin is the **shortest** distance from the threshold to any observation
> <br>
> ![[margin-diagram.svg | 400]]

<br>

---
##### What is a Hyperplane? 

>[!check] Hyperplane
>- A hyperplane is a **linear decision surface** that splits the space into two parts
>- Clearly, it is a binary classifier
>- A hyperplane in $\mathbb{R}^n$ is an $n-1$ dimensional **subspace**
><br>
>![[hyperplanes.svg|400]]

##### What is a Support Vector? 

> [!check] Support Vector
> - The support vector are the **data points** that determine the position and orientation of the decision boundary (hyperplane) between different classes.
> - Note that it is not a line, but a specific **data point**.
> <br>
> ![[support-vec-diagram.svg|400]]

<br>

---

##### What is the weakness of Maximum Margin Classifiers? 

They are extremely sensitive to **[[1.2 Easy Questions#outlier|outliers]]** in the **training data**. 

<br>
##### ↳ So, do MMC's have high bias or high variance? 

Deciding whether or not to allow misclassifications by loosening the margin is an example of the **bias vs. variance tradeoff** that plagues all of machine learning. 

Since MMC's are very sensitive to the training data, they have **low bias** and **high variance**.
☞ They partition the training data perfectly (low bias)
☞ As a result this makes them susceptible to misclassifying real data (high variance).

This diagram should illustrate this: 

<br>

---
##### Most High variance

##### How do you know which Soft Margin is best? 

You should use **cross validation**. 

---

##### If you allow misclassifications, is it still a Maximum Margin Classifier? 

No. Picking a threshold which allows misclassifications instead of just maximising the margin means that our **maximum margin classifier** evolves into a **soft margin classifier**. 

⁂ A **soft margin classifier** is another name for a **support vector classifier**...


<br>

---
##### What is the idea behind Support Vector Classifiers? 

- In practice, real data is messy and cannot be separated perfectly with a hyperplane.
- Furthermore, Maximal Margin Classifiers are extremely sensitive to outliers, for example: 

(Since the MMC simply sets the threshold value as the **halfway point** between `class_a` and `class_b`)



> [!check] Introducing the **Support Vector Classifier**
> - The idea is to draw a line (or plane) through the data. 
> - This divider is called the 'threshold', and observations are classified based on which side of the margin they fall.
> - Perhaps the most simple idea to segregate data, and yet some surprisingly deep math behind it.
> - This is a prerequisite for [[(2.9.2) Support Vector Machines|Support Vector Machines]]

<br>




##### Based on the dummy data, explain which out of MMC, SVC and SVM would be the best model to use. 