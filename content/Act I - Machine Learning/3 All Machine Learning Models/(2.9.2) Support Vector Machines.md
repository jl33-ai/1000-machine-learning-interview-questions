> [!quote]

---

##### What is a Support Vector Machine? 

> [!check] Non-Linearly Separable Data
> - The need for SVM's arises when dealing with non-linearly separable data - i.e data that you cannot separate with a [[(2.9.1) Support Vector Classifiers#What is a Hyperplane?|hyperplane]] (a line or plane)
> - A one-dimensional example:
> 
> <br>
> 
> ![[non-linear-separable.svg|250]]
> 
> 
> - A two-dimensional example
> 
> <br>
> 
> ![[1*JVZ4FXVRlr1oN-4ffq_kNQ.png|250]]

<br>

---
##### So then how do SVM's solve this? What's the main idea?

1. Start with data in a relatively **[[1.2 Easy Questions#What is a 'dimension' of data in ML|low dimension]]**


<br>


![[svm-steps.svg|300]]


<br>


2. Transform the data into a **[[1.2 Easy Questions#What is a 'dimension' of data in ML|higher dimension]]**

<br>


![[svm-steps-3.svg|300]]

<br>

3. Find a **[[(2.9.1) Support Vector Classifiers|Support Vector Classifiers]]** that separates the higher dimensional data into two groups

<br>

![[svm-steps-2.svg|300]]


<br>


###### ↳ Is it a supervised or unsupervised machine learning model?

They are definitely supervised. The decision boundary is derived directly from the training data. 



<br>

###### ↳ Are they only useful for classification? 


No! They can also be used for regression *(more on this later)*.

<br>


---

##### When would this type of data arise (i.e data that cannot be linearly separated)'

<br>

![[non-linear-separable.svg|250]]

<br>

☞ Imagine that the **x-axis** represents the `dosage` of a particular medicine. 
☞ The two classes would be `cured` or `not cured`.
☞ And the yellow dots in the middle would represent the ideal `dosage` of medicine that should be taken to cure the patient; to little or too much is ineffective. 

<br>

---

##### In the [[(2.9.2) Support Vector Machines#So then how do SVM's solve this? What's the main idea?|previous example]], each datapoint was squared to form the new dimension. But in practice, how can you determine the best transformation?

**This is true:** how did we know to create a new axis consisting of $\text{dosage}^2$, instead of $\text{dosage}^3$, or $\text{dosage}^{0.5}+33$? The answer is **kernal functions**

> [!check] Kernel Functions
> - SVM's use kernel functions to systematically find [[(2.9.1) Support Vector Classifiers|support vector classifiers]] in higher dimensions. 
> - These kernel functions transform the data in such a way that the data can then be separated by the simpler [[(2.9.1) Support Vector Classifiers|support vector classifiers]] using a flat [[(2.9.1) Support Vector Classifiers#What is a Hyperplane?|hyperplane]]. 
> - Note: there are many different types of kernel functions to pick from. Kernel functions are a class of functions

<br>

##### ↳ Could you name a commonly used kernel function? 

Yes, the **polynomial kernel**. 

> [!check] Polynomial kernel
> $(x_1 \times x_2 + r)^d$
> It has two parameters
> - $r$ 
> - $d$ the degree of the polynomial 
> And returns a number representing the high dimensional relationship between the two points. 
> <br>
> The degree of the polynomial directly specifies the number of dimensions of the data. E.g, $d=3$ transforms the data into $3$ dimensions $(x, y, z)$, meaning a plane is required to separate the data into two classes. 
> ![[dequalsthree.svg|400]]

Another commonly used [[(2.9.2) Support Vector Machines#In the (2.9.2) Support Vector Machines So then how do SVM's solve this? What's the main idea? previous example , each datapoint was squared to form the new dimension. But in practice, how can you determine the best transformation?|kernel function]] is called the **Radial Basis Function (RBF)**

> [!check] Radial Basis Function
> - Finds Support Vector Classifiers in infinite dimensions 
> - Behaves like a weighted nearest neighbour model

<br>

---

##### What is the kernel trick?

> [!check] Kernel trick (simple definition)
> - The Kernel trick is essentially what we've already discussed; it lets you separate data that is not [[1.3 Hard Questions#Describe some examples of data that cannot be linearly separated|linearly separable]] by simply adding dimensions. 
> - Another aspect of the kernel trick is that you don't need to actually transform all the data into the higher dimensional space, which is computationally expensive. 
> - Instead, the kernel functions allow you to calculate their high-dimensional relationships, without ever actually transforming the data points.  

<br>

> [!check] Kernel trick (advanced definition)
> - The kernel trick is a mathematical technique used in machine learning to solve problems that are not linearly separable in their original space by implicitly mapping the input features into a higher-dimensional space. This allows the application of linear algorithms to solve non-linear problems without explicitly performing the computationally expensive transformation.
> - In simpler terms, imagine you have a set of points that you cannot separate with a straight line on a two-dimensional plane. The kernel trick allows you to lift these points into a higher-dimensional space (for example, by adding another dimension that you can think of as height) where you can separate them with a plane (or a hyperplane in even higher dimensions).
> - The beauty of the kernel trick is that you do not need to compute the coordinates of the points in this higher-dimensional space. Instead, you use a kernel function to compute the dot product (a measure of similarity) between the points as if they were in the higher-dimensional space. This dot product is all that’s needed to perform operations such as classification, regression, or clustering in the transformed space.

<br>

---

##### What type of data are SVM's well suited for classifying? 

**Support Vector Machines** are also good for classifying: 
☞ Data that a neural net would overfit
☞ High dimensional data
☞ Non-linear data, using [[(2.9.2) Support Vector Machines#What do you do if|Kernel Tricks]]

<br>

---

##### Why would you use an SVM when neural networks exist? 

- If low amounts of high dimensional data.
- If it's data with clear segments, that neural nets are likely to overfit. 
- SVM's are lightweight whereas neural networks are computationally expensive.
- SVMs are simpler to implement and tune compared to neural networks, which require careful tuning of numerous hyperparameters and long training times.

> [!quote] Anonymous Youtuber
> Everyone always jumps to neural networks, but often SVMs do the same classification work with no huge iterative training required and are simply a better solution much of the time.

<br>

---

##### What is the difference between a MMC, SVC, and a SVM? When should you use each? 

☞ **[[(2.9.1) Support Vector Classifiers#What is a Maximal Margin Classifier?|MMC]]:** If the data has no outliers or noise, and is linearly separable.
☞ **[[(2.9.1) Support Vector Classifiers|SVC]]:** If the data is linearly separable.
☞ **[[(2.9.2) Support Vector Machines#^What is a Support Vector Machine|SVM]]:** If the data is not linearly separable.

<br>

---

##### Can we talk a bit about the math behind these kernel functions? How does the [[(2.9.2) Support Vector Machines#↳ Could you name a commonly used kernel function?|polynomial kernel function]] allow you to calculate the high dimensional relationships between points? 

The polynomial kernel function: $(x_1 \times x_2 + r)^d$ works like this:

$x_1$ and $x_2$ are two data points that we want to know the relationship between. 

Let's let $r=\frac{1}{2}$ and $d=2$. 

Then, 

$$\begin{align*}
\left( x_1 \times x_2 + \frac{1}{2} \right)^2 &= \left( x_1 \times x_2 + \frac{1}{2} \right)\left( x_1 \times x_2 + \frac{1}{2} \right) \\
&= x_1 x_2 + x_1^2 x_2^2 + \frac{1}{4} \\
&= \left( x_1, x_1^2, \frac{1}{2} \right) \cdot \left( x_2, x_2^2, \frac{1}{2} \right)
\end{align*}$$
