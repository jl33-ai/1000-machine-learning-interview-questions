##### Supposed we have a binary classification task, $Y = \{0, 1\}$. Can you please construct a zero-bias supervised learning model on the provided data: $\left\{ (4.1, 2.3), (4.3, 2.4), (2.7, 3.3) \right\}$

We could *learn* the following function from the data: 
$$$$

<br>



<br>

###### So does this make it a perfect machine learning model? Why or why not? 



<br>

---





###### (A) You're training a Gaussian Naive Bayes model to predict whether someone will like matcha based on whether they boulder, their age, and whether they like boba tea. How can you figure out which of these features actually has predictive power? 

- Cross validation
- 


###### **How would you measure (from that one shitty amazon)**
`#recommended products purchased / total of #recommended products`

<p style="text-align:justify;">Although it would seem more straightforward at first glance, it wouldn't be a particularly favorable metric in this case. See, if a certain user makes several searches and/or purchases (which is inevitable, in practice), the underlying observations that contribute to the metric are correlated with one another, leading to a violation of the t-test's assumption of observations' independence.</p>

Think of an extreme case to understand the intuition; e.g., if all purchases and recommendations were to come from a single customer.

I think Conversion rate might be a better primary metric here , across the variants.

<br>

---

###### QX) **Describe a scenario where you had to predict customer drop-offs (churn) for a subscription service. How would you approach the problem?**

<p style="text-align:justify;">**A**: I'd begin by collecting data on user interactions, usage frequency, and any feedback. I'd structure the data to have one row per customer, labeling them based on their activity status. Features would include usage metrics from the recent months. For predicting churn, I'd use a binary classification model, considering variables like last login, frequency of use, and interaction with new features.</p>

<br>

---

###### QX) **How would you prepare data for a machine learning model that aims to predict when a customer might churn?**
   - **A**: I'd aggregate user activity statistics over recent months, marking periods of inactivity. Using historical data, I'd create labels based on when a user churned: within 2 weeks, 1 month, 3 months, 6 months, etc. This way, the model not only predicts if a user will churn but also provides a probable timeframe.

<br>

---

###### QX) **How would you now implement this using code?**

<br>

---

###### QX) **Describe a time when you had to solve a problem using SciPy or TensorFlow.**
   - **A**: Using TensorFlow, I developed a neural network to recognize handwritten digits. After gathering a labeled dataset, I pre-processed the images to normalize their size and intensity. The model consisted of convolutional layers followed by dense layers. With SciPy, I've employed its optimization techniques to refine the parameters of a logistic regression model.

<br>

---

###### QX) **If given a dataset with user interactions on an app, how would you structure this data to predict if a user would stop using the app within the next six months?**
   - **A**: I would structure the data with one row per user. Features would include metrics like average session duration, frequency of use, features interacted with, and feedback given. The target variable would be binary, indicating if a user became inactive in the subsequent six months.

<br>

---

###### Do you have any experience using Hadoop or Spark?

<br>

---

###### What assumptions are made in the following models?

<br>

---

###### Which techniques can be used to evaluate feature importance in a dataset?

Several machine learning models and techniques are used to identify the most important features and assess correlations between features:

1. **Decision Trees and Random Forests**: These models provide feature importance scores which can be used to gauge the relevance of each feature in predicting the target variable.

2. **Linear Regression Models**: They can be used to understand the relationship between each predictor and the response variable. The coefficients indicate the strength and direction of the correlation.

3. **Principal Component Analysis (PCA)**: PCA is used for dimensionality reduction and can help in understanding the correlation structure of the features by transforming them into principal components.

4. **LASSO Regression (L1 Regularization)**: LASSO can shrink the coefficients of less important features to zero, effectively performing feature selection.

5. **Ridge Regression (L2 Regularization)**: Though it doesnâ€™t perform feature selection like LASSO, it can indicate feature importance through the size of coefficients.

6. **Gradient Boosting Machines (GBMs)**: Similar to decision trees, GBMs can also provide feature importance scores. 

7. **Correlation Matrices and Heatmaps**: These are statistical tools rather than ML models but are commonly used to visualize and assess the degree of correlation between features.

---

###### Clearly there are so many models. What should you actually consider when 


