(quant)

---


##### What do we mean when we say a matrix is 'sparse'

##### What do we mean when we say a matrix has high cardinality
##### What is Singular Value Decomposition and how is it different from Principal Component Analysis (PCA)?
##### How can SVD be used to reduce the dimensionality of a dataset?
##### Explain how matrix factorization is utilized in recommendation systems.
##### What are the challenges in implementing matrix factorization techniques in large datasets?
##### How do linear transformations relate to machine learning models?
##### What is a basis in linear algebra and how does the concept apply in the context of machine learning?
##### Explain the concept of orthogonality and its relevance in regression analysis.
##### How is the method of least squares used in training machine learning models?
##### In the context of neural networks, how are linear algebra operations applied during forward and backward propagation?
##### Discuss how linear algebra is applied in natural language processing, particularly in the context of word embeddings.
##### Can you explain the concept of tensor decomposition and its application in machine learning?
##### How do you use linear algebra for optimizing machine learning algorithms, particularly in gradient descent?

##### Why can't you multiply vectors?

https://youtu.be/htYh-Tq7ZBI?si=0lBepSQq_sTP4ENk

##### Can you explain the Cauchy-Schwarz inequality? 

> [!quote] Cauchy-Schwarz inequality
> - An upper bound on the inner product between two vectors
> - Considered one of the most important and widely used inequalities in mathematics.
> $$
> \[ |\langle \mathbf{u}, \mathbf{v} \rangle|^2 \leq \langle \mathbf{u}, \mathbf{u} \rangle \cdot \langle \mathbf{v}, \mathbf{v} \rangle \]
> $$

##### What is singular value decomposition? 


##### What is the formula for calculating the covariance matrix of a dataset, given observation vectors $x_i$ 

The full formula for the covariance matrix $C$ of a dataset without assuming that the mean of the data is zero involves subtracting the mean from each data point before calculating the outer product. For a dataset with $n$ observations (data points) each having $p$ variables (dimensions), the formula for the covariance matrix is as follows:

$$
C = \frac{1}{n - 1} \sum_{i=1}^{n} (x_i - \bar{x})(x_i - \bar{x})^T
$$

Here:
- \( x_i \) is a vector representing the \( i \)-th observation in the dataset.
- \( \bar{x} \) is the mean vector of the dataset, where each element of \( \bar{x} \) is the mean of the corresponding variable across all observations.
- \( (x_i - \bar{x}) \) is the deviation of \( x_i \) from the mean.
- \( (x_i - \bar{x})^T \) is the transpose of the deviation vector.
- \( \frac{1}{n - 1} \) is a scaling factor used to get an unbiased estimate of the covariance when working with a sample from a larger population. For the entire population, this would be \( \frac{1}{n} \).

Each element \( C_{jk} \) of the covariance matrix \( C \) is calculated by the following formula:

\[ C_{jk} = \frac{1}{n - 1} \sum_{i=1}^{n} (x_{ij} - \bar{x}_j)(x_{ik} - \bar{x}_k) \]

Where:
- \( x_{ij} \) is the value of the \( j \)-th variable in the \( i \)-th observation.
- \( x_{ik} \) is the value of the \( k \)-th variable in the \( i \)-th observation.
- \( \bar{x}_j \) is the mean of the \( j \)-th variable across all observations.
- \( \bar{x}_k \) is the mean of the \( k \)-th variable across all observations.

This formula ensures that the covariance matrix reflects the true variability and relationships between the different variables in the dataset.