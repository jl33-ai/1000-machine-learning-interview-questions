Why is machine learning / DL essentially just linear algebra
all about representing data as numbesr
'models' are just complex mathematicaly expressions, operations 
(quote andrej())

Dense vectors and matrices
Cardinality 

---
##### What is a vector?

##### What is the difference between a vector and a matrix?




<br>

---

##### What is a Unit Vector?
$$
\mathbf{u} = \frac{\mathbf{v}}{\|\mathbf{v}\|}
$$
   - Explanation: A unit vector is a vector with a magnitude of 1. It represents the direction of a vector without considering its magnitude.


<br>

---

##### What is Vector Projection?
$$
\text{proj}_{\mathbf{b}}\mathbf{a} = \frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{b}\|^2} \mathbf{b}
$$
   - Explanation: The vector projection of \(\mathbf{a}\) onto \(\mathbf{b}\) is a vector that represents the component of \(\mathbf{a}\) in the direction of \(\mathbf{b}\).



<br>

---

##### What is Scalar Projection?
$$
\text{comp}_{\mathbf{b}}\mathbf{a} = \frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{b}\|}
$$
   - Explanation: The scalar projection is the length of the vector projection, representing how much of one vector goes in the direction of another.


##### What is a dot product?
$$
\mathbf{a} \cdot \mathbf{b} = a_1b_1 + a_2b_2 + \ldots + a_nb_n
$$
   - Explanation: The dot product measures the similarity between two vectors and is a scalar value.



<br>

---

##### What is the Magnitude (Norm) of a Vector?
$$
\|\mathbf{v}\| = \sqrt{v_1^2 + v_2^2 + \ldots + v_n^2}
$$
   - Explanation: This formula calculates the length or size of the vector.



<br>

---

##### What is the Cross Product?
$$
\mathbf{a} \times \mathbf{b} = [a_2b_3 - a_3b_2, a_3b_1 - a_1b_3, a_1b_2 - a_2b_1]
$$
   - Explanation: The cross product of two vectors results in a vector that is perpendicular to both and is used in 3D space to determine the area of parallelograms and the direction of normals.



<br>

---

##### How is Matrix Addition and Subtraction Defined?
$$
C = A \pm B \text{ where } c_{ij} = a_{ij} \pm b_{ij}
$$
   - Explanation: Matrix addition/subtraction combines or contrasts the elements of two matrices of the same dimensions.



<br>

---

##### What is Matrix-Vector Multiplication?
$$
A\mathbf{v} \text{ resulting in a vector}
$$
   - Explanation: This operation transforms the vector \(\mathbf{v}\) by the matrix \(A\), often representing a change of basis or a linear transformation.



<br>

---

##### What is Matrix-Matrix Multiplication?
$$
AB \text{ where } c_{ij} \text{ is the dot product of the } i\text{th row of } A \text{ and the } j\text{th column of } B
$$
   - Explanation: Matrix multiplication combines two matrices, transforming one matrix by another, often used to represent a series of linear transformations.



<br>

---

##### What is the Determinant of a Matrix?
$$
\text{For 2x2 matrix } A, \text{det}(A) = ad - bc
$$
   - Explanation: The determinant provides information about the scaling factor of the linear transformation represented by the matrix and its invertibility.



<br>

---

##### What is the Inverse of a Matrix?
$$
A^{-1} \text{ such that } AA^{-1} = A^{-1}A = I
$$
   - Explanation: The inverse of a matrix undoes the transformation represented by the matrix, applicable only for square matrices with non-zero determinants.



<br>

---

##### What is the Trace of a Matrix?
$$
\text{tr}(A) \text{ is the sum of diagonal elements of } A
$$
   - Explanation: The trace is the sum of eigenvalues and represents the sum of all linear transformations' scaling factors.



<br>

---

##### What is the Rank of a Matrix?
   - Explanation: The rank indicates the number of linearly independent rows or columns in a matrix, reflecting the dimensionality of the vector space spanned by its rows or columns.



<br>

---
##### How are Linear Equations Represented in Matrix Form?
$$
A\mathbf{x} = \mathbf{b}
$$
   - Explanation: This represents a system of linear equations, where \(A\) contains the coefficients, \(\mathbf{x}\) is the vector of variables, and \(\mathbf{b}\) is the vector of constants.



<br>

---

##### What does it mean for something to be a 'linear combination' of something else?



<br>

---

##### What makes 2 or more vectors 'linearly independent'



<br>

---



##### What is a tranpose?

##### What is pre/post multiplying

##### What is a 'basis'

##### What is a linear transformation

##### Do the percentage flip thing and ask to prove how

##### What is the form of a weighted average? 

##### Normalized weight (0-1) and 

##### What is a tensor? 

##### State the eigenvector/eigenvalue equation

$$A \mathbf{v} = \lambda \mathbf{v}$$

Where:
- $A$ is a square matrix.
- $\mathbf{v}$ is an eigenvector of $A$
- $\lambda$ is the eigenvalue corresponding to the eigenvector $\mathbf{v}$

###### What is it actually saying? 

This equation says that you can post-multiply matrix $A$ by the vector v, and the result is a scalar multiple of vector v again. 
###### Explain why its so important for Machine Learning and Deep Learning

- General optimisation; 
- Regularisation
- This equation is used in PCA in order to determine the principal components that capture the most variance in the data. 
- Finding the steady state distribution of a Markov Chain

<br> 
###### What is a vector?



###### QX) What is invariance?

Certainly! Here's a concise explanation of invariance in statistics, computer science, and machine learning:

1. Statistics:
   - Invariance Principle: It relates to estimators that retain their properties even when subjected to certain transformations.
   - Example: Consider the median as an estimator of central location. If you add a constant to each data point in a dataset, the median of the transformed data will be the median of the original data plus the constant. This makes the median invariant to shifts.
   - The variance is LOCATION INVARIANT (sketch)
   - The MLE is invariant: 
   - A function of a sufficient statistic is also sufficient 
   - Variance in invariant 
   - Formula: If \(M\) is the median of dataset \(X\), then the median of \(X + c\) (where \(c\) is a constant) is $M + c$.


2. Machine Learning:
   - Invariance: A model's ability to provide consistent outputs despite certain transformations to the input.
   - Example: A convolutional neural network (CNN) used for image recognition might be designed to recognize a cat whether it's zoomed-in, rotated, or shifted, making the CNN invariant to these transformations.
   - Formula: Let \(f\) be a model. If $f(x) = f(T(x))$ for a transformation \(T\) (like rotation or scaling), then \(f\) is invariant to \(T\).

I hope this provides a clear understanding of invariance in these domains!


<br>

---



###### What does it mean for something to be 'proportional to' something else? 

- covariance matrices 

---

- eigenvalues/vectors - what is the logic behind 

$$Av = \lambda v$$
$v$: The eigenvector (a vector)
$A$: The matrix which acts on $v$ to 'transform it'
$\lambda$: The eigenvalue (a scalar number)

So you find $v$ and $\lambda$ for a given matrix $A$, such that transforming (multiplying) the vector by $A$ does not change its direction, only its magnitude (by an amount $\lambda$. 



<br>

---

- PCA
- Vector space 


- What Python library would you use for matrix manipulation
- How are matrices used in page rankÂ 
- How can matrices be Used to measure the similarity of two images
- How are matrices used in advanced recommender systems
- How is linear algebra used in a CNN? 
	- The kernel slide thing (do a dot product practice)

https://youtu.be/ZTt9gsGcdDo?si=f8LVG7Vliir7Dcyw

