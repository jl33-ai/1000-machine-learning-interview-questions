###### QX) What does the chain rule tell you

<br>

###### ↳ How do you apply the chain rule to a function $f(g(x))$

<br>

---

###### (Q4) Fill in the blanks
*For the natural log, $log_e(x)=0$ at $x=$ \_\_\_\_. So it is positive for all values $x>$ \_\_\_\_. $log_e(x)=1$ at $x=$\_\_\_\_. $log_e(0)$ is equal to \_\_\_\_*

fix this just make it rows. 

You just have to be careful with $log$ functions - they can be cunning sometimes! E.g, $log_e(2)$ is actually a **negative number.** 

> ☞ The natural logarithm is pivotal in statistics, data science, and machine learning due to its properties of transforming multiplicative relationships into additive ones and stabilizing variances, facilitating mathematical tractability and model interpretability.

In statistics/probability contexts, **probabilities** often need to be **multiplied** and log functions allow us to transform $a\cdot b$ into $log(a) + log(b)$. One concrete example is **entropy calculations*. 

<br>

---
###### QX) How do you find the maximum of a function/curve? 

- Optimisation

<br>

###### Follow up: What does the second derivative tell you? 

- Whether or not your stationary point {$x : f'(x) = 0$} is a local **minimum** or local **maximum**
	- In other words, are you on a hill or in a trough
	- This is related solely to the **polarity** of the second derivative at $x$
- The 'pointedness' of the curve at the current point $x$
	- A larger magnitude indicates more pointy

###### Extra: What does the third derivative tell you? 


###### QX) What is associativity, commutativity and distributivity?

**Commutativity** 
$a \cdot b = b \cdot a$
$a + b = b + a$

**Associativity** 
$a \cdot (b\cdot c) = (a \cdot b) \cdot a$
$a + (b+c) = (a+b)+c$

**Distributivity**
$a \times (b + c) = (a \times b) + (a \times c)$



<br>

---

2 unknowns requires 2 equations type beat. 


<br>

---

###### Factorise the quadratic expression $2x^2+7x+6$


---

Sums 

---

Taylor series 