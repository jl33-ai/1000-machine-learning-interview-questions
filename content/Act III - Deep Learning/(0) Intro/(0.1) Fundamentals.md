The taxonomy

Used to solve extremely non-trivial problems. I mean just THINK about how you'd try and solve some of this stuff with `if/else` statements

We'll introduce some new terminology.


Hopefully by the end, you will understand these diagrams:
![[For-each-local-feature-both-self-attention-and-cross-attention-mechanisms-are-used-to.jpg| 450]]
![[cross-attention-perceiver-io.png| 450]]
![[stable-diffusion-architecture.png| 450]]
![[1*Mxla5xcK2xZAUCZ-S2-ulw.jpg| 450]]
![[diffusion-models.png| 450]]



---
###### (A) How could you use these methods for facial recognition? 

**Answer 1**: You could extract features like eyebrow colour, eyebrow shape, and then train an ensemble model like Adaboost to classify

**Answer 2**: Enter deep learning

Deep learning figures out what the best features are, and automatically extracts then.
They extracts features that humans could never even comprehend, nor extract themselves. 
And they are the best ones. 

The underlying framework is identical. To classify a fruit using explicit features, we might want the following things: 

For example, a model trained to classify cars may see these features
![[1*bIY_8FE7dJaGk1kjU7iVSg.png]]

A very general image recognition model trained a dataset like IMAGENET might see these: 
![[cnn-features.png]]

And a facial recognition model
![[auto-feature-deep-learning.png]]

Notice how these features could never be explicitly engineered by a human being. 
The deeper in the network, the less interoperable they become. 

This is where Deep Learning comes in. 
Automatic feature extraction

###### (A) Deep learning seems really good... what's the catch? 

Deep learning models are extremely **data hungry**

###### (A) Your manager comes to your desk and tells you that your great plan to fund $10 million worth of manual human data labelling just got cut by a tenth to $1 million. What's your new strategy? 

Active learning. 


---

###### Q6) **What is a corpus?**
too specialist
<br>


###### Q1) What is an embedding? 

<br>

Activation Function
Backprop
Decoder
Dimensions
Discrimanator
Epoch
Feature set
feedback loop
Generalisation
Heurisic
hidden Layer

Inference
Learning rate
loss
pretrained model
Sequence to sequence task
Sigmoid function
Softmax
Trasformer



(ascii or doodle, simple in middleof page)
Data structures, algorithms, and Data

- What is a parent teacher network
- What is distilling
- (data science book can't hate it's good)

Neural Networks -> LLM

---

###### Q1) **In your own words, explain what a neural network is**

- A neural network is a function approximator
	- A 'function' is anything that takes an input, and produces an output 
	- (e.g `Handwritten Digits -> Digit from 1-10`)
	- (e.g `A word prompt -> A 512x512px image`)
- It **approximates** the function using a network of much smaller, simpler 'neurons' which are themselves also a type of function (best seen with a diagram)
	- Multiple inputs + a bias, some activation function, single output
	- 
- How does it fine tune these individual functions so that the larger function of the whole network is accurate? **Backpropogation**

https://youtu.be/0QczhVg5HaI?si=RyXl_bpNOsxEf62o
https://youtu.be/aircAruvnKk?si=G11HUoz-xhKFY_nJ
https://youtu.be/bfmFfD2RIcg?si=yIdjlFMA8jLk6twc
![[Pasted image 20231022130359.png | 250]]

<br> 

---


###### QX) What is transfer learning? 

When you invest 10,000 hours into learning the drums, and then decide to try the piano, and pick it up in a few weeks (because you learnt rhythm, parts of music-reading, and hand eye coordination from the drums) - **that is transfer learning.**

> â˜ž Similarly, in machine learning, transfer learning involves taking a pre-trained model (a model trained on a large dataset) and using it as a starting point to train a new model on a related task or dataset. Instead of starting the learning process from scratch, you leverage the **patterns** and **knowledge** the model has already learned. E.g; maybe your image recognition model already knows what a dog is... so... 

<br>

###### QX) What are the synergies (benefits)?

1. Data efficiency - can achieve more with less labelled data 
2. Reduced training time 
3. Improved performance



<br>

---

###### QX) Are you familiar with Large Language Models? 



<br>


---

###### QX) Artem's video



<br>


---

###### QX) You are building a resume parsing model. Explain how you would employ Deep Learning in order to classify a string of text 'Florida ...' as either a `Job Title`,  `Place`, `...`

1. Create embeddings
2. The brains 
<br> 

---

