https://youtu.be/9uw3F6rndnA?si=HkYCMJlZmSudQDA6

---

Transformers do not make the same Markovian assumptions as [[Bigram models|bigram models]]. Instead, they can consider long-range dependencies and relationships between words in a sentence, regardless of their distance from each other. Transformers consist of encoder and decoder blocks that can process an input sequence and generate an output sequence.