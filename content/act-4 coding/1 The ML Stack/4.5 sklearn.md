Now you're ready for `sklearn`

Like leetcodes but for ML - I should make that. There's minimum like predictive performance requirements. 

---

https://scikit-learn.org/stable/


### What is `scikit-learn`?

> [!check] `scikit-learn`
> - Abbreviated to `sklearn`
> - A Python library (package) containing lots of useful machine learning functionality. 
> - `sklearn` gives you the ability to use any of the machine learning models covered in [[2.0 Preface|this section]], able to fit to the data in a single line of code. 
> 
> **More fun facts**: 
> - It user-friendly and efficient
> - Built on [[4.5 numpy|numpy]], [[4.5 scipy|scipy]], and [[4.4 matplotlib|matplotlib]]
> - Open source, commerically usable under the [[2.4 Git#Are you familiar with various licenses?|BSD License]]  


<br>

---

##### Can you explain the structure of the `sklearn` package?

Make sure you understand how Python libraries are structured by answering [[2.3 Pythonese (hard)#Can you explain the internal structure of a Python package? What is the difference between a package, module, and class? Use the example `from sklearn.ensemble import RandomForestClassifier` to assist your explanation|this question]]. 

```text
scikit-learn (sklearn)
│
├── base
├── cluster
│   ├── AffinityPropagation
│   ├── AgglomerativeClustering
│   ├── Birch
│   ├── DBSCAN
│   ├── KMeans
│   ├── MiniBatchKMeans
│   ├── OPTICS
│   ├── SpectralClustering
│   └── ...
├── datasets
├── decomposition
├── ensemble
│   ├── AdaBoostClassifier
│   ├── AdaBoostRegressor
│   ├── BaggingClassifier
│   ├── BaggingRegressor
│   ├── GradientBoostingClassifier
│   ├── GradientBoostingRegressor
│   ├── RandomForestClassifier
│   ├── RandomForestRegressor
│   └── ...
├── feature_extraction
├── feature_selection
├── linear_model
│   ├── LinearRegression
│   ├── LogisticRegression
│   ├── Ridge
│   ├── Lasso
│   └── ...
├── metrics
├── model_selection
│   ├── cross_val_score
│   ├── train_test_split
│   ├── GridSearchCV
│   ├── RandomizedSearchCV
│   └── ...
├── naive_bayes
├── neighbors
│   ├── KNeighborsClassifier
│   ├── KNeighborsRegressor
│   └── ...
├── neural_network
│   ├── MLPClassifier
│   ├── MLPRegressor
│   └── ...
├── pipeline
├── preprocessing
│   ├── StandardScaler
│   ├── MinMaxScaler
│   ├── LabelEncoder
│   ├── OneHotEncoder
│   └── ...
├── svm
│   ├── SVC
│   ├── SVR
│   ├── LinearSVC
│   ├── LinearSVR
│   └── ...
├── tree
│   ├── DecisionTreeClassifier
│   ├── DecisionTreeRegressor
│   └── ...
└── utils
```

<br>

###### ↳ By referring to this hierarchy, can you please write an import statement for importing a Support Vector Classifier under the alias of `AwesomeSVC`?

```python
from sklearn.svm import SVC as AwesomeSVC

myClassifer = AwesomeSVC() # then it can be instantiated like this
```

It's a little bit confusing, but hopefully this clarifies the internal structure of `sklearn`. 

<br>

---

##### Attempt this Q/A thread to load the `alex-boba.csv` dataset using `pandas`. 

After completing [[2.4 pandas#The following is the first 5 rows of a dataset `alex-boba.csv`. Alex visits various boba tea shops, sampling drinks with diverse characteristics such as price, volume, sugar level, and whether they are organic, and subsequently rates each drink on a scale of 1 to 10.|this question]], you have a dataset that looks like this in a `baba` DataFrame object. 


| Price ($) | Volume (ml) | Sugar Level (%) | Organic | Rating |
|-----------|-------------|-----------------|---------|--------------------|
| 4.74      | 399         | 1.9             | No      | Good                 |
| 5.58      | 477         | 61.8            | Yes     | Bad                  |
| 5.01      | 543         | 61.2            | No      | Bad                  |
| 4.72      | 585         | 61.7            | Yes     | Bad                  |
| 4.12      | 447         | 94.4            | Yes     | Bad                  |

###### ↳ Notice how in the `rating` column there are 'good' and 'bad' values. Instead of `good/bad`, can you replace it with a binary encoding of `0/1` using `LabelEncoder()`?

**Pay attention to these steps ([[2.2 Object Oriented Programming|instantiate]] -> fit_transform). It is essentially the same process for any model in `sklearn`**

```python
from sklearn.preprocessing import LabelEncoder

my_encoder = LabelEncoder() # Instantiate
boba['rating'] = my_encoder.fit_transform(boba['rating']) # Fit+Transform to the data
```


> [!check] `.fit()`
> 
> - The `.fit()` method is used for training or fitting the model to the given data. It computes the necessary parameters to apply a specific algorithm (like regression, classification, or clustering) to the data. 
> - For instance, in machine learning models, `.fit()` will involve learning the weights on training data.
> - It does not transform or alter the data, it only learns from it.

> [!check] `.transform()`
> 
> - The `.transform()` method is used to apply a transformation or an action learned from the `.fit()` method to the data. It's commonly used in preprocessing steps like scaling or normalizing data. 
> - For example, after computing the mean and standard deviation of the features in the training set using `.fit()`, `.transform()` applies the scaling (subtracting the mean and dividing by the standard deviation) to any dataset to standardize features.

> [!check] `.fit_transform()`
> 
> The `.fit_transform()` method combines the `.fit()` and `.transform()` methods into one step - because they are so commonly used together. 
> It first fits the model with the data (learns parameters) and then transforms the data accordingly. This is more efficient and often used in the preprocessing phase, like scaling or normalizing data, where it first learns the parameters (like mean, standard deviation for scaling) and then applies this transformation to the dataset in one go.
> Remember, something even as simple as a **binary label encoder** is still technically considered a 'model', thus it makes sense to use `.fit_transform().

<br>


###### ↳ Your next task is to split the `boba` dataset into two: the features and the response/target variable, represented by `X` and `y` respectively. 

> [!check] `X` and `y`
> - In machine learning, it is **convention** to represent the data as follows: 
> 	- `X`: features 
> 	- `y`: response variable


```python
X = boba.drop('rating', axis=1) # not in place
y = boba['rating']              # otherwise this wouldn't work
```


<br>

###### ↳ Why do you need to specify `axis=1`?

- The method `.drop()` will, by default, drop a row instead of a column. 
- In pandas, `axis=1` refers to columns, whereas `axis=0` (the default) refers to rows. 
- By setting `axis=1`, you're instructing pandas to drop a column, not a row. If you do not specify `axis=1`, pandas will look for a row with the label 'quality' to drop, which is not the intended action in this case.

<br>

###### ↳ Why do you need to specify `axis=1`?


---



<br>

---

#####

<br>

---

#####

<br>

---

#####

<br>

---

#####

<br>

---

#####

<br>

---

#####

<br>

---

#####

<br>

---

#####

<br>

---

#####

<br>

---

##### How does `StandardScalar()` work. 