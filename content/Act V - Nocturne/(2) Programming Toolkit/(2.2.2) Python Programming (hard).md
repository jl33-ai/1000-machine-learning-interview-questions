

What is currying

`:=` What is this operator? 

`~` What is this operator? Use it in `pandas` to...

#####  What is the difference between `&` vs `and`

#####  Explain the difference between an object and a class


---

##### I have a list of tickets. Every ticket can have any number of 'tags' (e.g `is_important`). List **three** different ways to represent your tickets in a `pandas` DataFrame.

1. **String of comma-separated values**

If I knew the number of tags per ticket wasn't very large, I would choose to store it as **space** or **comma-separated string**.

```
1. Storing Tags as Comma-Separated Strings:

    ticket_id    tags
    1           "tag1, tag2, tag3"
    2           "tag2, tag3"
    3           "tag1, tag3"
```

2. **Boolean Flags for Each Tag**

If I knew I was going to perform analysis on individual tags, such as aggregating or using it as input into a machine learning model, I would create a separate column, or **boolean flag**, for each tag. 
Then I would indicate the existence of a tag using `True` or `False`. This would result in a very **wide** DataFrame. 

```
2. Boolean Flags for Each Tag:

    ticket_id    tag1    tag2    tag3
    1            True    True    True
    2            False   True    True
    3            True    False   True
```

3. **Multi-hot Encoded Columns**

This is essentially the same idea as **boolean flags**, but instead of `True/False`, use `1` and `0`. This would be useful for analysis and machine learning applications further down the pipeline. 

```
3. Multi-hot Encoded Columns:

    ticket_id    tag1    tag2    tag3
    1            1       1       1
    2            0       1       1
    3            1       0       1
```

4. **Separate rows for each tag**

If the `ticket_id` didn't have to be [[(0.0) Databases Crash Course#^10cc0d|uniquely identifying]], then I could simply create a `tag` column and then use a new row for each new tag assigned to a ticket. In this case, each ticket will appear multiple times in the DataFrame, once for each of its tags.

```
4. Separate rows for each tag:

    ticket_id    tag
    1            tag1
    1            tag2
    1            tag3
    2            tag2
    2            tag3
    3            tag1
    3            tag3
```

<br>

---

##### What is serializing?

> [!note]
> **Serializing** refers to translating a **data structure** or **object state** into a form that can be stored outside of the program, and then reconstructed later. 

If you haven't encountered the need to serialize yet, you probably haven't done enough machine learning. 

<br>
##### ↳ List some objects/data that you might need to serialize as a data scientist. 

1. **Trained Machine Learning Models**: Once you've trained a model, you need to save it for later use, whether for making predictions, further training, or analysis. Serialization allows you to store the model's state.
2. **Data Preprocessing Pipelines**: These include encoders, normalizers, and other transformers that you've fit to your data. Saving these ensures consistency in data preprocessing in future model use.
3. **Data Sets**: Particularly large datasets that have been cleaned and preprocessed might be serialized for efficient reloading in future sessions.
4. **Configuration Files**: Storing hyperparameters, environment settings, or experiment setups in a serialized format can help in replicability and documentation of your work.
5. **Custom Objects**: If you have built custom classes or functions in your data science workflow, serializing them can be useful for reusability across different projects or sharing with colleagues.

<br>
##### ↳ Imagine you've just spent 7 hours and lots of \$\$\$ training a model in your Jupyter Notebook, only to realise you never implemented serialization. What do you do?

`>>> OUTPUT: Model successfully trained`

If I realized that I forgot to serialize a model after training, I would take the following steps:

**Check if the Model is Still in Memory**
1. **Interactive Python Environment (e.g., Jupyter Notebook, Python Shell)**:
    - If I ran my code in an interactive environment where the Python session is still active, the model might still be in memory. In this case, I could directly serialize the model by executing the serialization code (using `pickle`, `joblib`, or another method) in the same session.

```python
import pickle

# Assuming my_model is your model variable
with open('model_filename.pkl', 'wb') as file:
    pickle.dump(my_model, file)
```

2. **Command Line or Non-Interactive Environment**:
    - If I ran the script via the command line or a non-interactive environment (like executing a `.py` file), and the script has finished running, the Python process would terminate, and all variables (including my model) are cleared from memory. 
    - In this case, unfortunately, there's **no way** to serialize the model post hoc. The model would need to be retrained.

**This is a costly mistake, but it emphasizes the importance of integrating serialization into the workflow.**

<br>

---