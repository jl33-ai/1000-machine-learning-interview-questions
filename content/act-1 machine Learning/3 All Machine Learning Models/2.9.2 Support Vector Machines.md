> [!quote] The radial kernel works in infinite dimensions, I know that sounds kind of crazy, but it's actually not that bad - ***StatQuest***

> [!quote] The SVM is one of the most simple and elegant ways to tackle the fundamental machine learning task of classification. - ***StatQuest***


---

##### What is a Support Vector Machine? 

> [!check] Non-Linearly Separable Data
> - The need for SVM's arises when dealing with non-linearly separable data - i.e data that you cannot separate with a [[2.9.1 Support Vector Classifiers#What is a Hyperplane?|hyperplane]] (a line or plane)
> - A one-dimensional example:
> 
> <br>
> 
> ![[non-linear-separable.svg|250]]
> 
> 
> - A two-dimensional example
> 
> <br>
> 
> ![[1*JVZ4FXVRlr1oN-4ffq_kNQ.png|250]]

<br>

---
##### So then how do SVM's solve this? What's the main idea?

1. Start with data in a relatively **[[1.2 Easy Questions#What is a 'dimension' of data in ML|low dimension]]**


<br>


![[svm-steps.svg|300]]


<br>


2. Transform the data into a **[[1.2 Easy Questions#What is a 'dimension' of data in ML|higher dimension]]**

<br>


![[svm-steps-3.svg|300]]

<br>

3. Find a **[[2.9.1 Support Vector Classifiers|Support Vector Classifiers]]** that separates the higher dimensional data into two groups

<br>

![[svm-steps-2.svg|300]]


<br>


###### ↳ Is it a supervised or unsupervised machine learning model?

They are definitely supervised. The decision boundary is derived directly from the training data. 



<br>

###### ↳ Are they only useful for classification? 


No! They can also be used for regression *(more on this later)*.

<br>


---

##### When would this type of data arise (i.e data that cannot be linearly separated)'

<br>

![[non-linear-separable.svg|250]]

<br>

☞ Imagine that the **x-axis** represents the `dosage` of a particular medicine. 
☞ The two classes would be `cured` or `not cured`.
☞ And the yellow dots in the middle would represent the ideal `dosage` of medicine that should be taken to cure the patient; to little or too much is ineffective. 

<br>

---

##### In the [[2.9.2 Support Vector Machines#So then how do SVM's solve this? What's the main idea?|previous example]], each datapoint was squared to form the new dimension. But in practice, how can you determine the best transformation?

**This is true:** how did we know to create a new axis consisting of $\text{dosage}^2$, instead of $\text{dosage}^3$, or $\text{dosage}^{0.5}+33$? The answer is **kernal functions**

> [!check] Kernel Functions
> - Kernel functions allow for more flexible decision boundaries by implicitly transforming data into higher dimensions. 
> - SVM's use kernel functions to **systematically** find [[2.9.1 Support Vector Classifiers|support vector classifiers]] in higher dimensions. 
> - These kernel functions transform the data in such a way that the data can then be separated by the simpler [[2.9.1 Support Vector Classifiers|support vector classifiers]] using a flat [[2.9.1 Support Vector Classifiers#What is a Hyperplane?|hyperplane]]. 
> - Note: there are many different types of kernel functions to pick from. Kernel functions are a class of functions

<br>

##### ↳ Could you name a commonly used kernel function? 

Yes, the **polynomial kernel** and the **radial basis function**.

> [!check] The polynomial kernel
> $$
>(x_1 \times x_2 + r)^d
> $$
> - $x_1$ and $x_2$ are the two datapoints that we want to know the high-dimensional relationship between. 
> - $r$ the polynomial's coefficient
> - $d$ the degree of the polynomial 
> And returns a number representing the high dimensional relationship between the two points. 
> <br>
> The degree of the polynomial directly specifies the number of dimensions of the data. E.g, $d=3$ transforms the data into $3$ dimensions $(x, y, z)$, meaning a plane is required to separate the data into two classes. 
> ![[dequalsthree.svg|400]]

Another commonly used [[2.9.2 Support Vector Machines#In the (2.9.2) Support Vector Machines So then how do SVM's solve this? What's the main idea? previous example , each datapoint was squared to form the new dimension. But in practice, how can you determine the best transformation?|kernel function]] is called the **Radial Basis Function (RBF)**

> [!check] Radial Basis Function
> $$
> e^{-\gamma(x_1-x_2)^2}
> $$
> - Finds Support Vector Classifiers in infinite dimensions 
> - Behaves like a weighted nearest neighbour model

<br>

---

##### What is the kernel trick?

> [!check] Kernel trick (simple definition)
> 1. The Kernel trick is essentially what we've already discussed; it lets you separate data that is not [[1.3 Hard Questions#Describe some examples of data that cannot be linearly separated|linearly separable]] by simply adding dimensions. 
> 2.  Another aspect of the kernel trick is that you don't need to actually transform all the data into the higher dimensional space, which is computationally expensive. Instead, the kernel functions allow you to calculate the relationship between two points - which is all you need to know to find the hyperplane - without ever actually transforming the data points. 

<br>

> [!check] Kernel trick (advanced definition)
> - The kernel trick is a mathematical technique used in machine learning to solve problems that are not linearly separable in their original space by implicitly mapping the input features into a higher-dimensional space. This allows the application of linear algorithms to solve non-linear problems without explicitly performing the computationally expensive transformation.
> - In simpler terms, imagine you have a set of points that you cannot separate with a straight line on a two-dimensional plane. The kernel trick allows you to lift these points into a higher-dimensional space (for example, by adding another dimension that you can think of as height) where you can separate them with a plane (or a hyperplane in even higher dimensions).
> - The beauty of the kernel trick is that you do not need to compute the coordinates of the points in this higher-dimensional space. Instead, you use a kernel function to compute the dot product (a measure of similarity) between the points as if they were in the higher-dimensional space. This dot product is all that’s needed to perform operations such as classification, regression, or clustering in the transformed space.

<br>

---

##### What type of data are SVM's well suited for classifying? 

**Support Vector Machines** are also good for classifying:
☞ Small datasets
☞ Data that a neural net would overfit
☞ High dimensional data
☞ Non-linear data, using [[2.9.2 Support Vector Machines#What do you do if|Kernel Tricks]]

<br>

---

##### Why would you use an SVM when neural networks exist? 

- If low amounts of high dimensional data.
- If it's data with clear segments, that neural nets are likely to overfit. 
- SVM's are lightweight whereas neural networks are computationally expensive.
- SVMs are simpler to implement and tune compared to neural networks, which require careful tuning of numerous hyperparameters and long training times.

> [!quote] Anonymous Youtuber
> Everyone always jumps to neural networks, but often SVMs do the same classification work with no huge iterative training required and are simply a better solution much of the time.

<br>

---

##### What is the difference between a MMC, SVC, and a SVM? When should you use each? 

☞ **[[2.9.1 Support Vector Classifiers#What is a Maximal Margin Classifier?|MMC]]:** If the data has no outliers or noise, and is linearly separable.
☞ **[[2.9.1 Support Vector Classifiers|SVC]]:** If the data is linearly separable.
☞ **[[2.9.2 Support Vector Machines#^What is a Support Vector Machine|SVM]]:** If the data is not linearly separable.

<br>

---

##### Can we talk a bit about the math behind these kernel functions? How does the [[2.9.2 Support Vector Machines#↳ Could you name a commonly used kernel function?|polynomial kernel function]] allow you to calculate the high dimensional relationships between points? 

The polynomial kernel function: $(x_1 \times x_2 + r)^d$ works like this: $x_1$ and $x_2$ are two data points that we want to know the high-dimensional relationship between. Let's let $r=\frac{1}{2}$ and $d=2$. Then,

$$
\begin{align*} \left( x_1 \times x_2 + \frac{1}{2} \right)^2 &= \left( x_1 \times x_2 + \frac{1}{2} \right)\left( x_1 \times x_2 + \frac{1}{2} \right) \\ &= x_1 x_2 + x_1^2 x_2^2 + \frac{1}{4} \\ &= x_1^2 x_2^2 + x_1 x_2 + \frac{1}{4} \\ &= \left( x_1, x_1^2, \frac{1}{2} \right) \cdot \left( x_2, x_2^2, \frac{1}{2} \right) \end{align*}
$$

Now, notice how the two vectors in the [[1.4 Linear Algebra#What is a dot product?|dot product]] represent the point vectors (coordinates) of the points in the higher dimension? Note that we can ignore the $\frac{1}{2}$ at the end, as this component of the vector will always be a double up. 

$$
\left(x_1, x_1^2, \frac{1}{2}\right) \to \left(x_1, x_1^2\right)
$$

So this is how the [[2.9.2 Support Vector Machines#↳ Could you name a commonly used kernel function?|polynomial kernel function]] maps the data points to their positions in [[1.2 Easy Questions#What is a 'dimension' of data in ML|higher dimensions]]. 

All you need to do is plug values into $x_1$ and $x_2$ in the **kernel** to get the high dimensional relationships between two points. Notice how if you take the start and end of the kernel equation, the data is never actually transformed. This, again, is the [[2.9.2 Support Vector Machines#What is the kernel trick?|kernel trick]]. 

<br>

---

##### Can you please go through the math for the [[2.9.2 Support Vector Machines#↳ Could you name a commonly used kernel function?|radial basis function]]?

###### Step 1

$$
e^{-\gamma(x_1-x_2)^2} = \text{the high-dimensional relationship}
$$

The radial kernel (radial basis function (RBF)) determines how much influence each observation has on classifying new observations. 

For any two observations $x_1$ and $x_2$, we get the squared distance between them with $(x_1-x_2)^2$

$\gamma$ is simply a scalar parameter which scales this squared distance, and thus scales the influence. It is determined via [[1.2 Easy Questions#Cross Validation|cross validation]].

Since this squared distance function is part of an $e^{-x}$ function, clearly the larger the squared distance between two observations, the lower the value of the $RBF$, due to the decaying nature of $e^{-x}$. 

###### Step 2

Now, we must take a step back and consider a polynomial kernel function again. If we let the coefficient $r=0$, then the function simplifies to: 

$$
(x_1 \times x_2 + r)^d = (x_1 \times x_2)^d = (x_1^d \cdot x_2^d) = (x_1^d)\cdot (x_2^d)
$$

Notice how the vectors in the resulting dot product only have **one component**. Thus, when $r=0$, all the polynomial kernel does is shift the data down its original axis, by raising it to the power $d$. 

I.e $x_1 \to x_1^2$

So setting $r$ to $0$ seems pointless, since the dot products leave the data in their original dimension. 


###### Step 3

If we add the $r=0, d=1$ case to the $r=0, d=2$ case, the result becomes: 
$$
x_1 x_2 + x_1^2  x_2^2 = (x_1, x_1^2) \cdot (x_2, x_2^2)
$$

If we let $d \to \infty$

$$
x_1 x_2 + x_1^2  x_2^2 + x_1^3 x_2^3 + \dots = (x_1, x_1^2, x_1^3, \dots) \cdot (x_2, x_2^2, x_2^3, \dots)
$$

Notice how we can convert this series into a dot product. 

###### Step 4

Now, let's revisit the original radial basis function and expand the quadratic in the power. 

$$
e^{-\gamma(x_1-x_2)^2} = e^{-\gamma(x_1^2 + x_2^2 - 2x_1x_2)} = e^{-\gamma(x_1^2 + x_2^2)}e^{\gamma 2x_1x_2}
$$

Letting $\gamma = \frac{1}{2}$ to simplify the $2ab$

$$
= e^{-\frac{1}{2}(x_1^2 + x_2^2)}e^{x_1x_2}
$$

###### Step 5 

Now let's find the Taylor Series Expansion of the last term, $e^{x_1x_2}$. We know that the central ($a=0$) Taylor series expansion of $e^x$ is:

$$
e^x = 1 + \frac{1}{1!}x + \frac{1}{2!}x^2 + \frac{1}{3!}x^3 + \ldots + \frac{1}{\infty!}x^\infty
$$

Now simply let $x = x_1x_2$ to get

$$
e^{x_1x_2} = 1 + \frac{1}{1!}{x_1x_2} + \frac{1}{2!}({x_1x_2})^2 + \frac{1}{3!}({x_1x_2})^3 + \ldots + \frac{1}{\infty!}({x_1x_2})^\infty
$$

###### Step 6

Notice that each term in this Taylor Series Expansion contains a polynomial kernel with $r=0$ and $d = 0, 1, \dots, \infty$

Again, we can change the series into a dot product, but this time with square rooted coefficients so that it all works out. 

$$
e^{x_1x_2} = \left(1, \sqrt{\frac{1}{1!}}x_1, \sqrt{\frac{1}{2!}}x_1^2, \sqrt{\frac{1}{3!}}x_1^3, \ldots, \sqrt{\frac{1}{\infty!}}x_1^\infty \right) \cdot \left(1, \sqrt{\frac{1}{1!}}x_2, \sqrt{\frac{1}{2!}}x_2^2, \sqrt{\frac{1}{3!}}x_2^3, \ldots, \sqrt{\frac{1}{\infty!}}x_2^\infty \right)
$$


###### Step 7

Now, substitute this $e^{x_1x_2}$ back into the original radial basis function, and let $b=e^{-\frac{1}{2}(x_1^2 + x_2^2)}$ for simplicity. Then: 

$$
e^{-\frac{1}{2}(x_1-x_2)^2} = \left( s, s\sqrt{\frac{1}{1!}}x_1, s\sqrt{\frac{1}{2!}}x_1^2, \ldots, s\sqrt{\frac{1}{\infty!}}{x_1}^\infty \right) \cdot \left( s, s\sqrt{\frac{1}{1!}}{x_2}, s\sqrt{\frac{1}{2!}}{x_2}^2, \ldots, s\sqrt{\frac{1}{\infty!}}{x_2}^\infty \right)
$$

Finally, we see that the radial kernel for points $x_1$ and $x_2$ is equal to the dot product of two vectors with an infinite number of dimensions. 

For example, take the datapoints `2` and `3`. Simply computing 
$$
e^{-\frac{1}{2}(2-3)^2}=0.6065
$$ 
gives us their infinite-dimensional relationship, encapsulated within a **single value**. 


<br>

---

##### How would you go about deploying a SVM on a dataset about (), using `Python`? 




