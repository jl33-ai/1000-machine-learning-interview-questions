**Helpful to have already answered:**
☞ [[1.3 Hard Questions#What is the "bias versus variance" tradeoff in ML?|The bias/variance tradeoff]]
☞ [[1.3 Hard Questions#cross valida]]|Cross validation]

---
##### What is a Maximal Margin Classifier?  

> [!check] Maximal Margin Classifier
> **The idea** is to simply draw a line through the data which separates it into two classes. A maximal margin classifier simply takes the halfway point between `class_a` and `class_b`, whereas a Support Vector Classifier uses a more robust approach, by allowing for some misclassifications. 
> 
> **A few more definitions:**
> ☞ Also known as a **hard** margin classifier (does not allow any misclassifications in training)
☞ The maximal margin classifier is a **crude, hypothetical** model which motivates the need for the Support Vector Classifier. 
☞ The result of using a threshold which maximises the margin.
☞ The maximal margin classifier is the hyperplane with the maximum margin, $max(M)$ subject to $||β||=1$

<br>

---
##### What is a margin?

>[!check] Margin
> The margin is the **shortest** distance from the threshold to any observation
> <br>
> ![[margin-diagram.svg | 400]]

<br>
###### What is a hard margin?


> [!check] Hard margin
> A hard margin is a [[2.9.1 Support Vector Classifiers#What is a hyperplane?|hyperplane]] that can perfectly separate the data into two classes without any misclassification.
> **Hard margins can only exist under two conditions:** 
> 1. The data is linearly separable
> 2. There are no outliers
> 
> Therefore, if either of these are not satisfied, the hard margin cannot exist. 
> ![[hard-margin.svg|400]]





<br>

---
##### What is a hyperplane? 

>[!check] Hyperplane
>- A hyperplane is a **linear decision surface** that splits the space into two parts
>- Clearly, it is a binary classifier
>- A hyperplane in $\mathbb{R}^n$ is an $n-1$ dimensional **subspace**
><br>
>![[hyperplanes.svg|400]]


<br>


> [!check] Flat Affine Subspaces
> (From mathematics) 
> - Instead of saying **a point**, A flat, affine, 0-dimensional subspace 
> - Instead of saying **a hyperplane**, A flat affine subspace 

<br>

---
##### What is a Support Vector? 

> [!check] Support Vector
> - The support vector are the **data points** that determine the position and orientation of the decision boundary (hyperplane) between different classes.
> - Note that it is not a line, but a specific **data point**.
> <br>
> ![[support-vec-diagram.svg|400]]

<br>

---

##### What is the weakness of Maximum Margin Classifiers? 

They are extremely sensitive to **[[1.2 Easy Questions#outlier|outliers]]** in the **training data**. 

Sometimes, the data is not linearly separable due to **noise** and **outliers**
<br>
##### ↳ So, do Maximal Margin Classifier's have high bias or high variance? 

Since MMC's are very sensitive to the training data, they have **low bias** and **high variance**.
☞ They partition the training data perfectly (low bias)
☞ As a result this makes them susceptible to misclassifying real data (high variance).

Deciding whether or not to allow misclassifications by loosening the margin into a [[2.9.1 Support Vector Classifiers#↳ What is a soft margin?|soft margin]] is an example of the **bias vs. variance tradeoff** that plagues all of machine learning. 

<br>

---

##### If you allow misclassifications, is it still a Maximum Margin Classifier? 

No. Picking a threshold which allows misclassifications instead of just maximising the margin means that our **maximum margin classifier** evolves into a **soft margin classifier**. 

⁂ A **soft margin classifier** is another name for a **support vector classifier**...

<br>

---
##### What is the idea behind Support Vector Classifiers? 

- In practice, real data is messy and cannot be separated cleanly with a hyperplane - there may be overlap.
	- This type of data is one kind of [[1.3 Hard Questions#Describe some examples of data that cannot be linearly separated|non-linearly separable]] data
- Maximal Margin Classifiers are extremely sensitive to outliers, for example: 

(Since the MMC simply sets the threshold value as the **halfway point** between `class_a` and `class_b`)



> [!check] Introducing the **Support Vector Classifier**
> - The idea is to use a **[[2.9.1 Support Vector Classifiers#↳ What is a soft margin?|soft margin]]** instead of a **hard margin** to separate the data. 
> - This divider is called the 'threshold', and observations are classified based on which side of the margin they fall.
> - Perhaps the most simple idea to segregate data, and yet some surprisingly deep math behind it.
> - This is a prerequisite for [[2.9.2 Support Vector Machines|Support Vector Machines]]

<br>

###### ↳ What is a soft margin? 

> [!check] Soft margin
> A soft margin is a [[2.9.1 Support Vector Classifiers#What is a hyperplane?|hyperplane]] that allows some misclassification by introducing slack variables that allow some data points to be on the wrong side of the margin.
>  ![[soft-margin.svg|400]]



<br>

---
##### Most High variance

##### How do you know which Soft Margin is best? 

You should use **cross validation**. 














---

##### Based on the following dummy data, explain which out of MMC, SVC and SVM would be the best model to use. 


<br>

![[mmc-svc-svm.svg|500]]

<br>

**a)** A Maximal Margin Classifier

☞ Because the data is clean and **linearly separable**.

**b)** A Support Vector Classifier / Soft Margin 

☞ Because the data contains **outliers** which would throw off a MMC.

**c)** A Support Vector Machine

☞ Because the data **cannot** be linearly separated. Read on to the next section, [[2.9.2 Support Vector Machines|Support Vector Machines]] to learn how... 

<br>


---

 