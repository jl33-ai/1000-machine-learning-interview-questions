> [!quote] The first step is to formulate the machine learning task as a mathematical optimization problem. 

Gradient descent is an optimization algorithm used primarily to minimize a function.

---

##### What is a convex optimization problem? 

> [!check] Convex Optimization Problem
> This refers to a specific subset of optimization problems where the objective function is convex, and the feasible region (if any constraints exist) is also a convex set. Convex problems have the property that any local minimum is also a global minimum, which makes them particularly nice for optimization.


![[1701176474304.jpg]]


<br>

---

##### What is a non-convex optimization problem? 

---

https://youtu.be/iudXf5n_3ro?si=Jndenbr03_bTb-8w
##### What is the goal of gradient descent? 

The name of the game is simply to find the values of the weights and biases which minimize the cost function. 

Adam Optimisation
Stochastic gradient Descent

<br>

---

##### What might the cost function be for this neural network: 

https://youtu.be/qg4PchTECck?si=_jYLIrrHJoXosfuB
<br>

---

##### Why do we need an algorithm for this? Can't we just plot the function and point to the minimum? 




<br>

--- 

##### Iterative

![[convex-optimization-explained-with-examples.png]]