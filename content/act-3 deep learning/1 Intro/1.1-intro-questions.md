> [!check] People get into machine learning and think there's nthing left to do because we have tensor flow. 

https://github.com/Sroy20/machine-learning-interview-questions/blob/master/list_of_questions_deep_learning.md
---

The taxonomy

Used to solve extremely non-trivial problems. I mean just THINK about how you'd try and solve some of this stuff with `if/else` statements

We'll introduce some new terminology.


Hopefully by the end, you will understand these diagrams:
![[For-each-local-feature-both-self-attention-and-cross-attention-mechanisms-are-used-to.jpg| 450]]
![[cross-attention-perceiver-io.png| 450]]
![[stable-diffusion-architecture.png| 450]]
![[1*Mxla5xcK2xZAUCZ-S2-ulw.jpg| 450]]
![[diffusion-models.png| 450]]

##### Is it always best to use deep learning model over a traditional model, if possible? 

No. Read [[2.0 Preface#What is the point in using a traditional model when neural networks exist|this question]].

---
###### (A) How could you use these methods for facial recognition? 

**Answer 1**: You could extract features like eyebrow colour, eyebrow shape, and then train an ensemble model like Adaboost to classify

**Answer 2**: Enter deep learning

Deep learning figures out what the best features are, and automatically extracts then.
They extracts features that humans could never even comprehend, nor extract themselves. 
And they are the best ones. 

The underlying framework is identical. To classify a fruit using explicit features, we might want the following things: 

For example, a model trained to classify cars may see these features
![[1*bIY_8FE7dJaGk1kjU7iVSg.png]]

A very general image recognition model trained a dataset like IMAGENET might see these: 
![[cnn-features.png]]

And a facial recognition model
![[auto-feature-deep-learning.png]]

Notice how these features could never be explicitly engineered by a human being. 
The deeper in the network, the less interoperable they become. 

This is where Deep Learning comes in. 
Automatic feature extraction

###### (A) Deep learning seems really good... what's the catch? 

Deep learning models are extremely **data hungry**

###### (A) Your manager comes to your desk and tells you that your great plan to fund $10 million worth of manual human data labelling just got cut by a tenth to $1 million. What's your new strategy? 

Active learning. 


---

##### Q6) What is a corpus?
too specialist
<br>


##### What is an embedding? 

<br>

Activation Function
Backprop
Decoder
Dimensions
Discrimanator
Epoch
Feature set
feedback loop
Generalisation
Heurisic
hidden Layer

Inference
Learning rate
loss
pretrained model
Sequence to sequence task
Sigmoid function
Softmax
Trasformer



(ascii or doodle, simple in middleof page)
Data structures, algorithms, and Data

- What is a parent teacher network
- What is distilling
- (data science book can't hate it's good)

Neural Networks -> LLM

---

###### Q1) **In your own words, explain what a neural network is**

- A neural network is a function approximator
	- A 'function' is anything that takes an input, and produces an output 
	- (e.g `Handwritten Digits -> Digit from 1-10`)
	- (e.g `A word prompt -> A 512x512px image`)
- It **approximates** the function using a network of much smaller, simpler 'neurons' which are themselves also a type of function (best seen with a diagram)
	- Multiple inputs + a bias, some activation function, single output
	- 
- How does it fine tune these individual functions so that the larger function of the whole network is accurate? **Backpropogation**

https://youtu.be/0QczhVg5HaI?si=RyXl_bpNOsxEf62o
https://youtu.be/aircAruvnKk?si=G11HUoz-xhKFY_nJ
https://youtu.be/bfmFfD2RIcg?si=yIdjlFMA8jLk6twc
![[Pasted image 20231022130359.png | 250]]

<br> 

---


###### QX) What is transfer learning? 

When you invest 10,000 hours into learning the drums, and then decide to try the piano, and pick it up in a few weeks (because you learnt rhythm, parts of music-reading, and hand eye coordination from the drums) - **that is transfer learning.**

> ☞ Similarly, in machine learning, transfer learning involves taking a pre-trained model (a model trained on a large dataset) and using it as a starting point to train a new model on a related task or dataset. Instead of starting the learning process from scratch, you leverage the **patterns** and **knowledge** the model has already learned. E.g; maybe your image recognition model already knows what a dog is... so... 

<br>

###### QX) What are the synergies (benefits)?

1. Data efficiency - can achieve more with less labelled data 
2. Reduced training time 
3. Improved performance



<br>

---

###### QX) Are you familiar with Large Language Models? 



<br>


---

###### QX) Artem's video



<br>


---

###### QX) You are building a resume parsing model. Explain how you would employ Deep Learning in order to classify a string of text 'Florida ...' as either a `Job Title`,  `Place`, `...`

1. Create embeddings
2. The brains 
<br> 

---

##### What is fine tuning? 

> [!check] Finetuning 
> Finetuning is no different than training, we just make sure to initialize (the weights and biases) from a pretrained model and train with a smaller learning rate.

<br>

---

##### What is the soft max function? 

##### What is the argmax function

The `argmax` function is related to, but distinct from, the 'max' function. While the 'max' function returns the maximum value in a set, the `argmax` function returns the *index* of the maximum value. This is a subtle but important difference. Let's explore how `argmax` works:

### How `argmax` Works

1. **Inputs**: 
   - The primary input to `argmax` is a collection of values, typically an array or a tensor. This collection can be multi-dimensional.
   - Optionally, you can specify the dimension (or axis) along which to find the index of the maximum value. If not specified, `argmax` typically operates on the flattened array.

2. **Process**:
   - `argmax` scans through the given collection of values.
   - It keeps track of the highest value found and its index.
   - If the collection is multi-dimensional and a specific dimension is specified, `argmax` is applied along that dimension. For example, in a 2D array, specifying a dimension of 0 will apply `argmax` to each column, and specifying a dimension of 1 will apply it to each row.

3. **Outputs**:
   - The output of `argmax` is the index (or indices, in the case of a multi-dimensional array) of the maximum value.
   - The nature of the output depends on whether the input is a flat array or a multi-dimensional array and whether a specific dimension was specified. 
   - In a flat array, the output is a single index. In a multi-dimensional array, the output can be an array of indices, one for each row or column (or along another dimension, if specified).

### Example

Consider a 2D array:

```
[
  [1, 3, 5],
  [2, 4, 1]
]
```

- Using `argmax` without specifying a dimension will return the index of the maximum value in the flattened array, which in this case is `5` with a flat index of `2`.
- If `argmax` is used with `dim=0` (along columns), it will return `[1, 1, 0]`, indicating the indices of the maximum values in each column.
- With `dim=1` (along rows), it returns `[2, 1]`, indicating the indices of the maximum values in each row.

### In Summary

`argmax` is particularly useful in machine learning and data analysis contexts where not just the maximum value is important, but also the location (index) of that value in a dataset. In classification tasks, for example, the index represents the class label, so `argmax` is used to convert network outputs (scores/probabilities for each class) into predicted class labels.

<br>

---
##### Why is cross entropy used instead of the sum of squared residuals (SSR) as loss function for neural nets. 

	It works by... treating the output. Must be softmax'ed first, is argmax by default. 

- Log? 
- Why is it simiplified form? 
- Used when . works for binary classes 
- what is used for nn's which output a value

[![Cross-Entropy Loss Function. A loss function used in most… | by Kiprono  Elijah Koech | Towards Data Science](https://miro.medium.com/v2/resize:fit:1400/1*60s9Kiwpm-QZBh0F1NK9eg.png)![Cross-Entropy Loss Function. A loss function used in most… | by Kiprono  Elijah Koech | Towards Data Science](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAXQAAACICAMAAADAmuC3AAAAhFBMVEX////8/Pz39/fz8/PGxsbr6+v5+fnDw8Pe3t7o6Ojx8fHW1tbAwMDt7e3l5eXQ0NCxsbGoqKja2tqgoKCurq66urqGhoadnZ2Xl5eMjIySkpJ+fn5xcXF3d3dnZ2ekpKRhYWEAAABPT09ZWVlOTk4/Pz9HR0c1NTUqKioTExMtLS1DQ0MAFHANAAAXlUlEQVR4nO1dC5ucqNIuFBUVwRveL53pmdlzvv3//++rArunbzOZ7GaTs4nvszttFBVeioIqCgTYsWPHjh07duzYsWPHjh07duzYsWPHPwzJu5+dhd8PBpafnYXfEFr/7Bz8ftBV9bOz8Puh1OnPzsKOHf8sPD8LCFmW+D87L78NzJfG5Iiie/2S/ezM/DYYv7DtSAzvpfFvfnd8Bnln3r32n+PpyPamtgriOby8+0R2WP8DeftlUfbvX1Nf1ot/SW5/+guhjuX5kIvvnLFfGe1Hxk/95Y1KNbmUSLoyKO08MVCFwNLQnX9XA+24w5P7iauiKNY7TXP470mtQ3xQ9rf3yhxGmGEJoAGIWiNyOt/8kOz+GnhFOi+1NAT9MAz9ZhKxNwXjE8OYsg8PIZhoQdrpPzZjN0vC/oGa2nGNtEMlgr+qaxH57WVvPh8aLnxSNj00GaypjGIn3TN0KoWd9M9DHZpifn/4AoezdoGoSFMkOT7mbOAt9OPMSaeLSeZJl+ykfzdMydU/RbWNXBgIrKkRlIQVBR76GHj547P3S6I/O7ta+ze7aBO1zHEImauJBpImYcWPztwvCsO3AzWv76VJbn53/D3oL68vhP/78uVL+PXkO74HPD9x8P3ds7Ljl0bpJDzmX0m34zti86IH48/Nxm8FPXpVUVTe7lb5gQi7rKjrYif9RyIYPIYAPu6+8h07duzYseMbod6Lpzv7BHz2TorHyekojf9epn51tOp0lM2Xvpf8LfDoHd9jJS/+od7cwXkCap++/hAFwOi5wyF4O51dTOmlWyTBNqIst9iXuL14zOWEEjK+LzH4EAWIp00ZDBnoCH95bKDyIVFqmybaGHWSrZpNqFUFmcH2UJZaBVh3egvSgM7bSb+EnZjuLiemC9DNNjUxJHUZNij4kyDr1Eyi2MIx3PXI/vWPGTBqG6ryO+gSbqI8izREdesPdpZDpjvpj5FZ+ksk3XDwG2J3SF5R1yQL6ZsRIJxgTe1aJCI9atd5bVH02QhhXqBQq6LVUHaGpzXkJXi9709SY3rOd9IvERc14rIHLJDiEnLS14N/BJg8w5WNuhAFHIBRl7hJunP96lz5OhnoWTkHXZcSK4D+XyCPJN6yS/pXUUAnY8iR52Q2cWEM9E0jofVhXfMUwpOkw0mnpyTl1IfWY9hiQ0iXvscKgOQY4TBHYltod53+FRQUacG6U6gjQ61BkUXYl070bzUyKLeL5SkJNPnWmYYUoUHdb+8CT2U/7KOXr6OicWIoLhyMRlJ8QJ6ONKhRwr8fp2PytyF9mcuaxukD1YNS2GoCyPZx+oeI34se+kaLNLk8KoMPUu7YsWPHjn8K/C5uGk5elrvZu4fTeQ/6hMfTfng2HPZ4JgIOEDMc+A2X3EvHWnATXK0FZPVN5F3s3GHyMnxDKghq7+5NIb4q3YN+CcjEhMOT9GLZHDsN+FbHsNq8v3hazDeiurhVSOyCdIaD9PI2HcFkVG87iHT5qi3pmqcopdIIzcE3wniQOht0G8SX5PBqrsj064OwLoRwBMHxOpc6shtqXKfDtsOpmfyepMdtVVVu+QuzcRdIuvWLp7k20KpWcAO1ArnysAePrjAinYbq1j/jyNzuReGl+kItFDZeDzpPi7CBIr5Lp8qFDNz+9yT9DdlK6+wEkl7ThF2ajwL0KqOitKsuZlp56iPprKi7rqa1ACtpGUumb++l22j+Q9REuqkhXLK+LKDNTulO77COx9fflvQk4ojLNaUFzIwT6Wj7r2mUoj1psLeboAkgcQ6ATadHxHJz3UEuTBOhqF6CHso2pi7YtYjrdCOojpT9b0n6PVCnS+wvTQFGRiCbYWJxDWUT6fMoZtPpPop72ZmrxRg16nFaoaQ7X8sc1DhMSqH2Sgd5pdSnCHVQyXfSHa4XEeEgxKQ4HnHT1dsoxt+k9p3Njy48ij22perBomrPhkliu0l30gnD5YwGqZ+UBiU5qZLoJpDCf7go8mpEHpGuye7SWQsMCQ/nnfQdO3bs2LFjx44dO3bs2LFjxw/Fp1eKqA/cFup26vGdxOkn9nVNbjcT/Mx+JHcZuLxfXz5BnvMQfWrzmPLMj3bpvbvNDm+9aELdpriFbB+cNFtQ1dPpBIshvpz7vXys8iA7wjXi643/wy3iKiefovx4F1HvcBXR5a9c3qTg813dxTcZeHsHEtJcyNXAssN2JORnvF/67LNsyY9WRHCAeKZCn7MTXcb9ZRmo6WsPVe6O69C1C8rcBapMFzvrpObSe0r+7KvX0C3X79XKnRY3fN8FzNGJkV1eHTKYb9P29w3mwdcd7A3kmXTLl9z9E30KwmK+3uPxYewenvS709Gp8htKWr2RTisX3u42yjqo33miPpbHYDDBrHtgbST50vIyp6mysDq4JhQ8Ma4rfEq4DClgQh9qTS2j/KMOp2oOXiAfk5dVwIHbBUPmwNY65zTDD2OkXkGbKiPh7VKg02WulmytxZG3Ak/0NWZMyYIkrjyqdDR6YJjlvIGj8iq7zxTK/UIFLHQFZnIbmTrS/Tzn7FDgY9OcH+iMHPjqyGgLltO2hNBNEnJ6tYw6YmE+5jbmVKvnPP2j9paCt7yLoeY1mCdYOG0B2fEO5mqmLPveQeMz6ygH3mAWeoNVxbHQheoaGDE73jJ4Ob63GBtB60SqEg66YVv2bzHB5HNQPcxep70FsIU+h4pqX0ZbDE/PehPTFAxNgR0wE+XgDSS1KFg0F9MwNtl5yiNEdrb5gHJUeBTbj3xjqmMo+k0Y6vAJxIqi7rd4tWxRd9nmUGphJWNQqZ/7fUqSPkMbz8ojJo+OdG5A0H3sjfREJkfMEr776CSdiQUMD4j2oMoWq41p29JcY4rFc2FNaffq0s+UWzARvJK7vaZM53Yalub/aqlzmuHzxrADH+kPn0nSea5ykvTRFnpxjaZQyGE4M9cg24DYaNXosn8LY6KCk3oZw5GLjCh4UYKy3PghWD3YhIk8kBzWIsYaNjrqhGKW3DiKSBOEM75f4fsjm4EcC9GIheHDMI8L/OkeuKoATx9BDJOd2xmJ9E5aFnRVWtKDxgAKHZGO+e/iJyWo55o8WFClrRHtB7Ppg952aXEfH2hW+8A20mn30jFOSAHGlac6emyJuiXH/PkHJfACS0mpvFABLOmx0fiX58xm2nuyc3wRB75qDqir2YF1EB6gUUcqUDmcSJ+x0L3bbXlVpPeftmidLoix8JU6CvFo1MD+gGMCqFyWWLVYl08MWuERd0ucFpbDKZlB2GB7LtkBKtr9VtOzZl/XNa0S0kfolYRnlBL70P+A7uCpQB7SIvwPG2KfolWwtsoBXiJeeU8MZf/AeI8dE7faP0ndPpmHEh/TdPGSYK6PKYpabt9sVaTXwyqSxQp6OKX02+rsGLWcvaKQqBe6IKtgKG3FpD3eToow6CLoNFCamGajDiGS3iqsniPWZlxWEq/lKyrNpzoZ4JWqM2qxfHlOullKVAG5Rt3zwvkKvc8rmDI8R4UOXO89cJtQzTZzRWpC5ETTQqgHpKMwx9SWFNegIi+lenNfvlGogSmHAU8z7lY0c7/EhBxCaVtggnIQecC4iDIWMcEFj+w7FT3DFylXkCJOn9LhCaTaE5xnXGhMXGoeV6WmFskiVVpFHDBQ3NP4UvxVHIR7onZDqSTCJzrRSrm0QsRLhVmiMCL8Y+MzxGkzXs5jwe1TUxVG9rINJgBlCjyNYkylzjTQNcwW5jMRXDMbqMAlZjeKfMxaCSxxvCTKT3iADw64LnmAhYbQLvfzoohRQs3dkA6LYJkS/Jv33mtvx2nfHX6Xie+7SiX5E7VDNv7tbzx0j0bR92jMwxHP/zaY+M6zxFn8XfbKZOqrxo3FX/9uSrDvP/mjkdJw6itge0j3t8N/V8fVYlFpce5j49f7mG6yWGWj0q9tmTg9TPDoed7xvfyw8j6upbJa3w4lvor68S7fD3xL8f02yanzeDzaBDXP77TBeMqp/06v1L/7gbIYOuxzi7MibB6R1NAowfta5xRFj84+pKp+b+Vboue7c5klcqSx/VfhPd7t7v6hoA/355wfYbhX5314v2e7PpfsnW7pwUsdTIbD+yjqnBWGNmfjedLQiEkyI09uu2HIIKxTfxAmhqSOQOWW4Nrw2mtjnduEUS5TNBENigUaiWFtM9WPPC3SQuFgdxPVEkeiRQC6zoBCD6XUdENmNu+Iy2qABhY3VK7MmGEj5JUurGnBywLy2g4bZB5DioZPpSOrI/0ezVY8E8dFGlehtoGkapgfiLpjmMka4q7MKWbV1YN8vW9sB/dytG3JlUEOmFKutjiYfZ17XYnEIB0hc6X2c2GEJK/PA83NUzR0fSFKIr1cMSNN2CbcxB1EhYCzvmhf/oSsRWMRUyxQo/lYU2ninhbji9X5p6IOMzewXCd4GW2bNLcUk0U8Mx42bqlE1qAIFIHXWSucCsBHlrQNy2zNhY70ETqBRmOt2ey+1JOQrWxrMTPcT3uP0VmsmgkPDqBHsEaT9wRNzBo808dhJGonbl2waZiAQuOr/IJ0U8YDjKlHRjXaZfQIWn5t23tOiVu68Xl7OVqhYYjWW4Y3GXoMVjEZ3UMEtIWEqEcQq5UFYy3n5uHmQM2Fqurt6gUvq6oC2mfDpqdTO1PkL8DWXWUwxbRNwhLl9toi5kQXnbSrgCR5PNU6GDTbpf8a2Vom5qtyNTybiH3agR7/oKQX5gnypwqa1/IFLZHgcAgvSC+LJsV6y3VUk2eI3G1buVWlG422oaTnLNQ6cnOkRRwT3Y9W5iDsmaznMFQuZ3ivsD4e5nuEC9LDNm+QBST91Qq0VRttZi1Lm9YuGz5uL5f5wQ8HrFAUP+fGaZ415rOL8XGYgr1wJ9szKJIw8/LI1JkvlD3auSXWzRPotsaDmvwbTtdj+04jf4WKjHPMbDpu3vSii1HKCudZjoj0ZzCr0sfQJiP2cGiUvLKJKLTlRGMoZkViJDyXEjoTwjDb3S7c13Q29fIMbe5N2OKTfvuw2gKh09dH0UsUOKtJV41GXwFPaVnbVUuk04cYC/KEVBjij3LmoWQ6l01W0xfy5AXpS5iNogmxrpdt6mAIeAyWdEmJbffjvj5Dnp9ZMyQ90DVYr5GkPVMXRzo2tXIrNZ2iKHn18PMd1eVsS2540BiU0E6vUkfGZE5RQl0XKxR93NR6FALHOqlTXZBx6iiNoWKwoUtHbYwefPrIS1C4756hTIB1Qai68J20yLBZvS6qdRvxMjcirEyWGzs+QD3jvHZSd3wsB+zlTT5T1Yu6cq3SQCmAVcbW+mrSsJPGrEM5UibypmwKr8UzoGPKmdWQK7//NlI9tvQ8XfOKN0aOMfZVMxVXo343N31jstr1NNBFsjb0ClRsK4lyitnXY9rUfBRhazJ/tbHz8WxfLGT+W3/WQBy/YSGjnOe/Zy/W+6pJQhh+A40s/HseFa/64NNCO3bsuOhk/YEGJ0kiKvCtKRne2mz5Q2P0c/gL27J8eEtZ56a4scDLT36nXT2yxzew9V1Pd/z9XFP6rJg6Gg5rmhxJI4gzUFe2fZlB8PGHzT5w38btN38TLas+/obDVLpdv05Qyae/61h/kNM2ua7Jk5xRqMG39Zv0oNijqR7qZ5jySqrugOY+kPSSuV2HWprlSnzMOv7pRUguoTSARNsaxsFSMlBlqxK8wHetQF0yvfmWgrsWAmS43597M+Ni9uAWdkN6fF2rs2CJZpxB5tmbe5Tfnl4dbhY2RQzh/36WxLRqD42lNAYVlnZbCFrQqhixSCFYW3BRgGNJu3OkF3v0rlRBRgPZNIPw4DPaTQK5sA90OfRC32MeTUvdI+zQkm+GHk1YiuQID7VCK6HGc17np2PSe6pI0RKt0XjPWyRdtuEkA92gOVCJgpEAJYsM/GM8MaykWv1RtiQQKp/jNzZdw4iayGzCIuyHGm1zzU7JajrVDh1iOYXGyMFwfnmLbXzhNenRkOvL0cLYNsIbvB7MgUJH/EmisYPjceidO6wJeq/3C0V5LWs038UqCjEOwQQNx5I3bKkVlrBc4yHrSK7SPBvYRDOHwZ8abatOcZ4eODaMVsTPOqSYDH+NxYsYXPWzRa6DPyMfj/RaFD1DJEHP3CavBBozhUDbBI3v1FBok5QLLfqutI8mO1rNQNulDOyZ52ax21eSCKDd2GdHzsnpwYmxWHI4WksY8qqYVvJg+DMar7y4cQBnj9SLenISls1QtuJWNzjSpa0kRtsKQjzK9iztsyjLXgAvfDc3Tz0SPqeoWz57tPYXFF/BI9saTXkKGHnRpsYMH0DyfOCjvwqYwgmCctEVZfeVXI9OY41Y+1gQNJlnKmVF+0aSe7CmbBzIGeQK0IoxiZeIPwj0Ej0cPTTfKSaH9AvmvNNIem3Q+CbS2wjtaw9Jb1N/daT3Hq2sRXs3SFzg3OhrLF6fHBiEriBgfTjzWZNuXcCMLd25oES1IqykJyfSczpVOHE/74Mz4YtdsJR6u+VWvUxQGPmmpmidKJIuc9/Nc/ZhijkpayIktGUGXhHpkw12WRWZ/8zGYUi0qPEObCDW3UHBONS1PtHHIc+kt3grkR6qRqVreACto6iAgDwJ5+Cig+4MxY08sA50ly5y6D0YtDXaqyLtgT7ZN6SDp7uyhzzXU5TneNrM4qjkxKIcm1ZYR5FuXIRcbrRc4sWoNpXqVXU25Owp7cMz6U5bpKMurD/iEkl9uNd754A9Menqrhf08uPVLdkxbe3ul24OQh0r1LQd5XiywxyZ8/goCqxy6/fEshq83AXqmQK0KBzImChtWvWnGjpMVPtzXj5ntZRBoW3gg6p0Gy5U4figfGKFrjWMhs2pmUQfld1AhCXiNe6H0PqAoS3TFbIufWQlYZcYWkea+w5xJWjjYKI/JuaoW0g8xoAFtFDcw6Qe9k9g/3r2rC01eGjweRQiikmsEy+xsnqSSFf9XUZO65vRJfPDu/FWeHZYVDTygBvBZsn1LTlF4RTRaUCHfVjo8u574Tl/NluxMzSprHi4naRzOD7wvHMiRschlRACN4bEkjlaQkpjC2+nMGNqCJldQ7890N/cwu5N7BPjYW/+OJj28+hIWIW8HGQFR9K68lNhshuSg3VUyg9HZN5CakPKz03f//P4GybLjh2/PNRtj5v87Xipv46f+e4fBgFsulWh5TaHu/UKDyItTutD7vVZXNyYB/rbPNX6/RUN0VcXO/xbgB1gcddvuSk04c6LB3Oto2NS3NtfLLkxfr6RqfADP8u/lXTaIJ7LAvLBravRLwbqGk9mRZFVFXfmBdpz+eot9GEEHDpX24jOCwg2rvjVSr+Y2rtLcEP6NsZmRX4a4zGblrRI2fGamgx9jyGddZdAaowlPVr5aVxlUwd2RHZNOquL6F8xd4AW2QAtjElI0dZ0ZtnWMS2eN2eNm+0FMnDLYps4HCl8mg5iI6W0k9/eRuxEkSE0nFX2kh0wOjNfpDZo2kUH0L/LFo6hpd23aUkx6SaJqeprP6dod56jKU4mN3hqgU5JW12SYMLtdbTQDR9MI3WBptz0b4ihtct3Wj5nSJvz1Szgo3pZ4A+RJqo6kR4+i9Sr7aoIKqpvxS/TBLprM37ZBKyxuig+X9ok/Uz6sHWMaHwfnTnv27TavdyQQMt8pWASbsrBkU5LRaYhtPe61Bek2yfbUAztIoL+1xH0oOMZpgJp7C1Hh6wsUrtgBqTqXAQPBdKQe91+gEisevtUa+gTyP4aQ+PVdCny3N6tb5duP2C8MG43PzvSHpb2McympfaRLC704U8oWkELUpBWF69S6KDb3AEutcvWtVxPiYySzwQ8/mwoo0DLTKeRiJxelmHEy0iQ75FHysZg6Ci2wmzsGEXGd4Z5LJ2/BC8F8nqMl8mrHRjtQhrqKMZI3DlVZO3UhjIZZkdEURia1MWdlPT67HpeR/ObLQB1inL/13j4F0B09zb31i0Ow9dljZOnDegrU9fjpOm9hdt2jVraNB9q7PT3+1Iy+4bht3fe2/IK/rtLu2PxiUlIIf4NveiOHTt27NixY8eOHTt27NixY8eOHb8I/h9R+0wjmm5pDgAAAABJRU5ErkJggg==)](https://www.google.com/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-loss-function-f38c4ec8643e&psig=AOvVaw3ooriIm0gYNW_A38B00arX&ust=1704435783211000&source=images&cd=vfe&opi=89978449&ved=0CBIQjRxqFwoTCLiy_aKMw4MDFQAAAAAdAAAAABAI)

<br>

---

##### Again, quickly state the purpose of regularization, then explain some ways that NN's are commonly regularized. 

Prevents overfitting... has the aim of making the model generalize better... more robust

Regularization in neural networks is a technique used to prevent overfitting, where the model performs well on the training data but poorly on new, unseen data. Overfitting is a common problem in complex models with many parameters, such as deep neural networks. There are several methods for regularization:

1. **L2 Regularization (Weight Decay)**: Adds a penalty term to the loss function proportional to the sum of the squares of the weights. This encourages the model to keep the weights small, which can lead to a simpler model that generalizes better.

2. **L1 Regularization**: Adds a penalty proportional to the absolute value of the weights. This can lead to sparse models where some weights become exactly zero, effectively performing feature selection.

3. **Dropout**: Randomly sets a fraction of input units to zero at each update during training, which helps prevent over-reliance on any one node and promotes distributed representations.

4. **Early Stopping**: Involves monitoring the performance of the model on a validation set during training and stopping the training process once the performance starts to degrade. This ensures the model doesn't overfit the training data.

5. **Data Augmentation**: Generating new training samples by altering existing ones (e.g., rotating, scaling, or cropping images in a vision task). This can provide more diverse data for training and helps in learning more general features.

6. **Batch Normalization**: Normalizes the input of each layer to have a mean of zero and a variance of one. This can stabilize and speed up training, sometimes reducing the need for dropout.

7. **Noise Injection**: Adding noise to inputs, weights, or even the outputs of layers during training can also act as a form of regularization, making the model more robust.

8. **Reducing Network Size**: Simplifying the architecture (fewer layers or nodes) can reduce overfitting since there are fewer parameters to learn.

9. **Ensemble Methods**: Combining the predictions from multiple models can improve generalization and reduce overfitting. Techniques include bagging, boosting, and stacking.

10. **Learning Rate Schedules**: Adjusting the learning rate during training (e.g., reducing it gradually) can help the network to converge to a better solution.

11. **Regularization via Attention Mechanisms**: In some architectures, particularly in sequence modeling and NLP tasks, attention mechanisms can help the model focus on the most relevant parts of the input, reducing overfitting.

Different methods can be combined depending on the specific problem and the nature of the dataset. The choice of regularization techniques is often empirical and may require tuning through cross-validation.

<br>

##### Can you explain the concept of `X` shot learning? (zero-shot, one-shot, two-shot, ...)


##### Can you explain the concept of transfer

https://chat.openai.com/share/f94c8196-6de7-4c66-b8c3-065d4a67987c




---